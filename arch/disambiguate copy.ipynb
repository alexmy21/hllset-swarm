{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AM-Guided Disambiguation Demo\n",
    "\n",
    "Demonstrates the new adjacency matrix-guided disambiguation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Imported binding MainInclude.eval was undeclared at import time during import to Main.\n",
      "WARNING: import of MainInclude.eval into Main conflicts with an existing identifier; ignored.\n",
      "WARNING: Imported binding MainInclude.include was undeclared at import time during import to Main.\n",
      "WARNING: import of MainInclude.include into Main conflicts with an existing identifier; ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HLLSet kernel from: None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "   \n",
    "from hllset_swarm.ingest import CorpusState\n",
    "from hllset_swarm.disambiguate import disambiguate_with_am \n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Simple Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ingesting 1 texts ===\n",
      "Text 1: ⊢人工智能⊣\n",
      "\n",
      "==================================================\n",
      "Processing window: tokens[0:3] → '⊢', '人', '工'\n",
      "Added to text HLL: '⊢' → hash 6626418808568176668\n",
      "Master HLL: '⊢' → hash 6626418808568176668\n",
      "Added to text HLL: '⊢人' → hash 2611208850325074467\n",
      "Master HLL: '⊢人' → hash 2611208850325074467\n",
      "Added to text HLL: '⊢人工' → hash 6662185350112548660\n",
      "Master HLL: '⊢人工' → hash 6662185350112548660\n",
      "\n",
      "==================================================\n",
      "Processing window: tokens[1:4] → '人', '工', '智'\n",
      "Added to text HLL: '人' → hash 2111899233249189501\n",
      "Master HLL: '人' → hash 2111899233249189501\n",
      "Added to text HLL: '人工' → hash 2558997374738549922\n",
      "Master HLL: '人工' → hash 2558997374738549922\n",
      "Added to text HLL: '人工智' → hash 538292936336947427\n",
      "Master HLL: '人工智' → hash 538292936336947427\n",
      "\n",
      "==================================================\n",
      "Processing window: tokens[2:5] → '工', '智', '能'\n",
      "Added to text HLL: '工' → hash 1872328049933700185\n",
      "Master HLL: '工' → hash 1872328049933700185\n",
      "Added to text HLL: '工智' → hash 3107484999921705844\n",
      "Master HLL: '工智' → hash 3107484999921705844\n",
      "Added to text HLL: '工智能' → hash 4599312375100631085\n",
      "Master HLL: '工智能' → hash 4599312375100631085\n",
      "\n",
      "==================================================\n",
      "Processing window: tokens[3:6] → '智', '能', '⊣'\n",
      "Added to text HLL: '智' → hash 2615720522607693844\n",
      "Master HLL: '智' → hash 2615720522607693844\n",
      "Added to text HLL: '智能' → hash 636610139397710922\n",
      "Master HLL: '智能' → hash 636610139397710922\n",
      "Added to text HLL: '智能⊣' → hash 3473476628305466539\n",
      "Master HLL: '智能⊣' → hash 3473476628305466539\n",
      "  Text HLL cardinality: 13\n",
      "\n",
      "Global state:\n",
      "  Total unique tokens: 14\n",
      "  Total HLLSets: 1\n",
      "  Total edges: 13\n",
      "  Master HLL cardinality: 14\n",
      "Original text: 人工智能\n",
      "HLL cardinality: 13\n",
      "AM:tensor(indices=tensor([[ 0,  2,  3,  3,  4,  4,  5,  6,  7,  7,  8, 12, 12],\n",
      "                       [10,  9,  7,  8,  3,  5,  1, 13,  0,  2, 11,  4,  6]]),\n",
      "       values=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
      "       size=(14, 14), nnz=13, layout=torch.sparse_coo)\n",
      "AM size: torch.Size([14, 14]), 13 edges\n"
     ]
    }
   ],
   "source": [
    "# Create corpus\n",
    "corpus = [\"人工智能\"]\n",
    "\n",
    "# Ingest\n",
    "state = CorpusState(P=10)\n",
    "state.ingest_corpus(corpus)\n",
    "\n",
    "# Get components\n",
    "adj, token_to_compact, hash_to_compact, compact_to_hash = state.get_adjacency_matrix()\n",
    "text_hll = state.get_hllset_for_text(0)\n",
    "print(f\"Original text: {corpus[0]}\")\n",
    "print(f\"HLL cardinality: {text_hll.count():.0f}\")\n",
    "print(f\"AM:{adj}\")\n",
    "print(f\"AM size: {adj.shape}, {adj._nnz()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting AM-guided disambiguation ===\n",
      "START symbol: '⊢'\n",
      "END symbol: '⊣'\n",
      "\n",
      "[Path 1] Current: '⊢', Collected: []\n",
      "  Valid 2-grams: []\n",
      "  ✗ Dead end (no valid 2-grams)\n",
      "\n",
      "=== Disambiguation complete ===\n",
      "Explored 1 paths\n",
      "Found 0 valid sequences\n",
      "\n",
      "==================================================\n",
      "Found 0 sequences:\n",
      "\n",
      "Best sequence: None\n",
      "Original:      人工智能\n",
      "Match: False\n"
     ]
    }
   ],
   "source": [
    "# Disambiguate - CORRECTED\n",
    "sequences, best = disambiguate_with_am(\n",
    "    text_hll,              # 1. HLLSet to disambiguate\n",
    "    adj,                   # 2. Adjacency matrix\n",
    "    token_to_compact,      # 3. token → compact_idx\n",
    "    compact_to_hash,       # 4. compact_idx → hash_value\n",
    "    state.hash_to_token,   # 5. hash_value → token (CORRECTED!)\n",
    "    state.lut,             # 6. LookupTable\n",
    "    max_paths=10           # 7. max_paths\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Found {len(sequences)} sequences:\")\n",
    "\n",
    "for i, seq in enumerate(sequences, 1):\n",
    "    print(f\"  {i}. {''.join(seq)}\")\n",
    "\n",
    "print(f\"\\nBest sequence: {''.join(best) if best else 'None'}\")\n",
    "print(f\"Original:      {corpus[0]}\")\n",
    "print(f\"Match: {best == list(corpus[0]) if best else False}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Multiple Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2 = [\"人工智能\", \"机器学习\", \"深度学习\"]\n",
    "\n",
    "state2 = CorpusState(P=10)\n",
    "state2.ingest_corpus(corpus2)\n",
    "\n",
    "adj2, token_to_idx2 = state2.get_adjacency_matrix()\n",
    "\n",
    "print(f\"Corpus: {corpus2}\")\n",
    "print(f\"Total tokens: {len(token_to_idx2)}\")\n",
    "print(f\"Total edges: {adj2._nnz()}\")\n",
    "print(f\"Master HLL: {state2.master_hll.count():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disambiguate each text\n",
    "for i, original_text in enumerate(corpus2):\n",
    "    hll = state2.get_hllset_for_text(i)\n",
    "    sequences, best = disambiguate_with_am(\n",
    "        hll, adj2, token_to_idx2, state2.lut, max_paths=20\n",
    "    )\n",
    "\n",
    "    recovered = ''.join(best) if best else 'FAILED'\n",
    "    match = '✓' if best == list(original_text) else '✗'\n",
    "\n",
    "    print(f\"\\\\nText {i+1}:\")\n",
    "    print(f\"  Original:  {original_text}\")\n",
    "    print(f\"  Recovered: {recovered} {match}\")\n",
    "    print(f\"  Paths explored: {len(sequences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create larger corpus\n",
    "large_corpus = [\n",
    "    \"人工智能是计算机科学的一个分支\",\n",
    "    \"机器学习是实现人工智能的一种方法\",\n",
    "    \"深度学习是机器学习的一个子领域\",\n",
    "    \"神经网络是深度学习的核心技术\"\n",
    "]\n",
    "\n",
    "state3 = CorpusState(P=12)\n",
    "state3.ingest_corpus(large_corpus)\n",
    "\n",
    "adj3, token_to_idx3 = state3.get_adjacency_matrix()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hllset-swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
