{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’re right – the **current inverted-index scaffold** is **over-engineered** for the **ordering task**.  \n",
    "We only need:\n",
    "\n",
    "1. **Look-up table** `<hash> → token` (1-,2-,3-gram) – **O(1)** cover extraction  \n",
    "2. **TF-based adjacency matrix** `A_M` – **weights for ordering** (ordering ≠ swarm)  \n",
    "3. **BSS lattice** `BSS_L` – **swarm trajectory only** (never used for order)\n",
    "\n",
    "Below is a **refactored pipeline** that **drops the inverted indices entirely** and **uses only**:\n",
    "\n",
    "- **hash table** for **cover extraction**  \n",
    "- **TF-weighted adjacency** for **ordering**  \n",
    "- **BSS lattice** stays **private to the swarm** – **never touched by ordering code**\n",
    "\n",
    "---\n",
    "\n",
    "## 1.  Data we *actually* need\n",
    "\n",
    "| Name | Structure | Purpose | Built when? |\n",
    "|---|---|---|---|\n",
    "| **HashTable** | `Dict[int, str]` | `<hash> → token` | **ingestion** |\n",
    "| **TF Matrix** | `sparse float32` | `weight = log(TF + 1)` | **ingestion** |\n",
    "| **BSS Lattice** | `sparse float32` | **swarm trajectory only** | **separate build** |\n",
    "\n",
    "**No inverted indices**, **no register/run slots**, **no bit-vectors for ordering**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.  Ingestion (new, 30 lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import hashlib\n",
    "from typing import List, Dict, Tuple, Set\n",
    "from collections import defaultdict\n",
    "\n",
    "# HyperLogLog parameters\n",
    "P = 10  # 2^10 = 1024 registers\n",
    "\n",
    "def hash32(s: str) -> int:\n",
    "    \"\"\"Compute 32-bit hash of string\"\"\"\n",
    "    return int(hashlib.md5(s.encode('utf-8')).hexdigest()[:8], 16)\n",
    "\n",
    "def slot(h: int) -> Tuple[int, int]:\n",
    "    \"\"\"Extract (register, run_of_zeros) from hash\"\"\"\n",
    "    reg = h & ((1 << P) - 1)\n",
    "    run = (h >> P).bit_length() if h >> P else 0\n",
    "    return reg, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LookupTable:\n",
    "    \"\"\"\n",
    "    Hash-based lookup table for tokens with collision handling.\n",
    "    \n",
    "    Structure:\n",
    "        hash -> {\n",
    "            'tokens': [list of tokens with this hash],\n",
    "            'hll_pair': (reg_num, run_zero)\n",
    "        }\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.table: Dict[int, Dict] = {}\n",
    "        self.token_to_hash: Dict[str, int] = {}\n",
    "    \n",
    "    def add_token(self, token: str) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Add token to LUT, return its HLLSet representation.\n",
    "        \n",
    "        Returns:\n",
    "            (reg_num, run_zero) pair\n",
    "        \"\"\"\n",
    "        h = hash32(token)\n",
    "        reg, run = slot(h)\n",
    "        \n",
    "        if h not in self.table:\n",
    "            self.table[h] = {\n",
    "                'tokens': [],\n",
    "                'hll_pair': (reg, run)\n",
    "            }\n",
    "        \n",
    "        if token not in self.table[h]['tokens']:\n",
    "            self.table[h]['tokens'].append(token)\n",
    "            self.token_to_hash[token] = h\n",
    "        \n",
    "        return reg, run\n",
    "    \n",
    "    def get_tokens_by_hll(self, reg: int, run: int) -> List[str]:\n",
    "        \"\"\"Get all tokens that map to this (reg, run) pair\"\"\"\n",
    "        result = []\n",
    "        for h, data in self.table.items():\n",
    "            if data['hll_pair'] == (reg, run):\n",
    "                result.extend(data['tokens'])\n",
    "        return result\n",
    "    \n",
    "    def get_hll_pair(self, token: str) -> Tuple[int, int]:\n",
    "        \"\"\"Get HLLSet representation for token\"\"\"\n",
    "        h = self.token_to_hash.get(token)\n",
    "        if h is not None:\n",
    "            return self.table[h]['hll_pair']\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hllset(lut: LookupTable) -> Set[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Build HLLSet from all registered tokens in LUT.\n",
    "    \n",
    "    Returns:\n",
    "        Set of (reg, run) pairs\n",
    "    \"\"\"\n",
    "    hllset = set()\n",
    "    for h, data in lut.table.items():\n",
    "        hllset.add(data['hll_pair'])\n",
    "    return hllset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_corpus(corpus: List[str]) -> Tuple[torch.Tensor, Dict[str, int], LookupTable]:\n",
    "    \"\"\"\n",
    "    Ingest corpus and build:\n",
    "    1. Lookup Table (LUT) with hash collisions and HLLSet pairs\n",
    "    2. Adjacency Matrix (AM) with frequency-based edges\n",
    "    \n",
    "    Returns:\n",
    "        adj: Sparse adjacency matrix\n",
    "        tok_id: token → AM node ID mapping\n",
    "        lut: LookupTable instance with hash and HLL data\n",
    "    \"\"\"\n",
    "    START = \"<START>\"\n",
    "    END = \"<END>\"\n",
    "    \n",
    "    lut = LookupTable()\n",
    "    tok_id = {START: 0, END: 1}\n",
    "    edge_freq = defaultdict(int)\n",
    "    \n",
    "    # Register special tokens\n",
    "    lut.add_token(START)\n",
    "    lut.add_token(END)\n",
    "    \n",
    "    for line in corpus:\n",
    "        text = line.strip()\n",
    "        if len(text) == 0:\n",
    "            continue\n",
    "        \n",
    "        # === START connection ===\n",
    "        first_char = text[0]\n",
    "        if first_char not in tok_id:\n",
    "            tok_id[first_char] = len(tok_id)\n",
    "        lut.add_token(first_char)\n",
    "        \n",
    "        start_2gram = START + first_char\n",
    "        if start_2gram not in tok_id:\n",
    "            tok_id[start_2gram] = len(tok_id)\n",
    "        lut.add_token(start_2gram)\n",
    "        \n",
    "        edge_freq[(tok_id[START], tok_id[start_2gram])] += 1\n",
    "        edge_freq[(tok_id[start_2gram], tok_id[first_char])] += 1\n",
    "        \n",
    "        # === Sliding window with step=2 ===\n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            g1 = text[i] if i < len(text) else None\n",
    "            g2 = text[i:i+2] if i+1 < len(text) else None\n",
    "            g3 = text[i:i+3] if i+2 < len(text) else None\n",
    "            \n",
    "            # Register all n-grams in LUT and AM\n",
    "            for gram in [g1, g2, g3]:\n",
    "                if gram and gram not in tok_id:\n",
    "                    tok_id[gram] = len(tok_id)\n",
    "                    lut.add_token(gram)\n",
    "            \n",
    "            # Build edges: 1g → 2g → 3g → next_1g\n",
    "            if g1 and g2:\n",
    "                edge_freq[(tok_id[g1], tok_id[g2])] += 1\n",
    "                \n",
    "                if g3:\n",
    "                    edge_freq[(tok_id[g2], tok_id[g3])] += 1\n",
    "                    \n",
    "                    if i+2 < len(text):\n",
    "                        next_g1 = text[i+2]\n",
    "                        if next_g1 not in tok_id:\n",
    "                            tok_id[next_g1] = len(tok_id)\n",
    "                            lut.add_token(next_g1)\n",
    "                        edge_freq[(tok_id[g3], tok_id[next_g1])] += 1\n",
    "            \n",
    "            i += 2\n",
    "        \n",
    "        # === END connection ===\n",
    "        last_char = text[-1]\n",
    "        if last_char not in tok_id:\n",
    "            tok_id[last_char] = len(tok_id)\n",
    "            lut.add_token(last_char)\n",
    "        \n",
    "        end_2gram = last_char + END\n",
    "        if end_2gram not in tok_id:\n",
    "            tok_id[end_2gram] = len(tok_id)\n",
    "        lut.add_token(end_2gram)\n",
    "        \n",
    "        edge_freq[(tok_id[last_char], tok_id[end_2gram])] += 1\n",
    "        edge_freq[(tok_id[end_2gram], tok_id[END])] += 1\n",
    "    \n",
    "    # Build sparse adjacency matrix\n",
    "    N = len(tok_id)\n",
    "    adj_u, adj_v, adj_w = [], [], []\n",
    "    \n",
    "    for (u, v), freq in edge_freq.items():\n",
    "        adj_u.append(u)\n",
    "        adj_v.append(v)\n",
    "        adj_w.append(float(freq))\n",
    "    \n",
    "    adj = torch.sparse_coo_tensor(\n",
    "        indices=torch.tensor([adj_u, adj_v], dtype=torch.long),\n",
    "        values=torch.tensor(adj_w, dtype=torch.float32),\n",
    "        size=(N, N)\n",
    "    ).coalesce()\n",
    "    \n",
    "    return adj, tok_id, lut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_ngrams(tokens: List[str], n: int) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Decompose n-grams into 1-grams.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of n-gram tokens\n",
    "        n: n-gram size (2 or 3)\n",
    "    \n",
    "    Returns:\n",
    "        Set of 1-gram characters\n",
    "    \"\"\"\n",
    "    result = set()\n",
    "    for token in tokens:\n",
    "        if len(token) == n:\n",
    "            for char in token:\n",
    "                result.add(char)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Cover extraction (hash table only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unambiguate_tokens(hllset: Set[Tuple[int, int]], lut: LookupTable, tok_id: Dict[str, int]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Disambiguate tokens using intersection of decomposed n-grams.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. T_1 = {all 1-gram tokens matching HLLSet pairs}\n",
    "    2. T_2 = {1-grams from decomposed 2-grams matching HLLSet}\n",
    "    3. T_3 = {1-grams from decomposed 3-grams matching HLLSet}\n",
    "    4. Return intersection T_1 ∩ T_2 ∩ T_3\n",
    "    \n",
    "    Returns:\n",
    "        Disambiguated token → node_id mapping\n",
    "    \"\"\"\n",
    "    # Collect tokens by n-gram type that match HLLSet\n",
    "    tokens_1g = []\n",
    "    tokens_2g = []\n",
    "    tokens_3g = []\n",
    "    \n",
    "    for reg, run in hllset:\n",
    "        matching_tokens = lut.get_tokens_by_hll(reg, run)\n",
    "        for token in matching_tokens:\n",
    "            if len(token) == 1:\n",
    "                tokens_1g.append(token)\n",
    "            elif len(token) == 2:\n",
    "                tokens_2g.append(token)\n",
    "            elif len(token) == 3:\n",
    "                tokens_3g.append(token)\n",
    "    \n",
    "    # Build sets\n",
    "    T_1 = set(tokens_1g)\n",
    "    T_2 = decompose_ngrams(tokens_2g, n=2)\n",
    "    T_3 = decompose_ngrams(tokens_3g, n=3)\n",
    "    \n",
    "    # Intersection\n",
    "    disambiguated_1g = T_1 & T_2 & T_3\n",
    "    \n",
    "    # Now rebuild full token set with disambiguated 1-grams\n",
    "    result = {}\n",
    "    \n",
    "    # Add disambiguated 1-grams\n",
    "    for token in disambiguated_1g:\n",
    "        if token in tok_id:\n",
    "            result[token] = tok_id[token]\n",
    "    \n",
    "    # Add 2-grams and 3-grams that are composed only of disambiguated 1-grams\n",
    "    for token in tokens_2g:\n",
    "        if len(token) == 2 and all(c in disambiguated_1g for c in token):\n",
    "            if token in tok_id:\n",
    "                result[token] = tok_id[token]\n",
    "    \n",
    "    for token in tokens_3g:\n",
    "        if len(token) == 3 and all(c in disambiguated_1g for c in token):\n",
    "            if token in tok_id:\n",
    "                result[token] = tok_id[token]\n",
    "    \n",
    "    # Always include START and END\n",
    "    result[\"<START>\"] = tok_id[\"<START>\"]\n",
    "    result[\"<END>\"] = tok_id[\"<END>\"]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ **O(cover size)** – **no bit-vector scan**, **no register/run**.\n",
    "\n",
    "---\n",
    "\n",
    "## 4.  Adjacency weights for ordering (TF-based)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_adjacency(adj: torch.Tensor, keep_token_ids: Dict[str, int], tok_id: Dict[str, int]) -> Tuple[torch.Tensor, Dict[int, int], Dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Prune adjacency matrix to only keep disambiguated tokens.\n",
    "    \"\"\"\n",
    "    keep_ids = set(keep_token_ids.values())\n",
    "    keep_list = sorted(keep_ids)\n",
    "    old_to_new = {old_id: new_id for new_id, old_id in enumerate(keep_list)}\n",
    "    \n",
    "    id_to_tok = {v: k for k, v in tok_id.items()}\n",
    "    new_to_tok = {new_id: id_to_tok[old_id] for old_id, new_id in old_to_new.items()}\n",
    "    \n",
    "    adj = adj.coalesce()\n",
    "    indices = adj.indices()\n",
    "    values = adj.values()\n",
    "    \n",
    "    mask = torch.tensor([\n",
    "        u.item() in keep_ids and v.item() in keep_ids\n",
    "        for u, v in zip(indices[0], indices[1])\n",
    "    ], dtype=torch.bool)\n",
    "    \n",
    "    new_indices = indices[:, mask]\n",
    "    new_values = values[mask]\n",
    "    \n",
    "    remapped = torch.tensor([\n",
    "        [old_to_new[u.item()] for u in new_indices[0]],\n",
    "        [old_to_new[v.item()] for v in new_indices[1]]\n",
    "    ], dtype=torch.long)\n",
    "    \n",
    "    pruned = torch.sparse_coo_tensor(\n",
    "        indices=remapped,\n",
    "        values=new_values,\n",
    "        size=(len(keep_ids), len(keep_ids)),\n",
    "        dtype=torch.float32\n",
    "    ).coalesce()\n",
    "    \n",
    "    return pruned, old_to_new, new_to_tok\n",
    "\n",
    "def reconstruct_order(pruned_adj: torch.Tensor, new_to_tok: Dict[int, str], start_id: int, end_id: int, \n",
    "                     total_tokens: int, k_threshold: float = 1.2, beam: int = 5, max_len: int = 100) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Reconstruct token order using beam search.\n",
    "    \n",
    "    Key changes:\n",
    "    - Allows duplicate tokens (no visited set)\n",
    "    - Stops when path length > k_threshold * total_tokens OR reaches END\n",
    "    - Open-ended parsing strategy\n",
    "    \n",
    "    Args:\n",
    "        pruned_adj: Pruned adjacency matrix\n",
    "        new_to_tok: Node ID → token mapping\n",
    "        start_id: START node ID\n",
    "        end_id: END node ID\n",
    "        total_tokens: Total disambiguated tokens count\n",
    "        k_threshold: Multiplier for max path length (default 1.2)\n",
    "        beam: Beam width for search\n",
    "        max_len: Hard limit on path length\n",
    "    \n",
    "    Returns:\n",
    "        List of candidate token sequences\n",
    "    \"\"\"\n",
    "    # Build adjacency dict\n",
    "    indices = pruned_adj.coalesce().indices()\n",
    "    values = pruned_adj.coalesce().values()\n",
    "    \n",
    "    adj_dict = defaultdict(list)\n",
    "    for i in range(indices.shape[1]):\n",
    "        u, v, w = indices[0, i].item(), indices[1, i].item(), values[i].item()\n",
    "        adj_dict[u].append((v, w))\n",
    "    \n",
    "    # Calculate stopping threshold\n",
    "    length_threshold = int(k_threshold * total_tokens)\n",
    "    \n",
    "    # Beam search - no visited set, allow duplicates\n",
    "    candidates = [([start_id], 0.0)]\n",
    "    \n",
    "    for step in range(max_len):\n",
    "        next_cand = []\n",
    "        \n",
    "        for path, score in candidates:\n",
    "            current = path[-1]\n",
    "            \n",
    "            # Stopping conditions:\n",
    "            # 1. Reached END token\n",
    "            # 2. Path length exceeds threshold\n",
    "            if current == end_id or len(path) > length_threshold:\n",
    "                next_cand.append((path, score))\n",
    "                continue\n",
    "            \n",
    "            # Explore neighbors (allow revisiting nodes for duplicates)\n",
    "            for next_node, weight in adj_dict.get(current, []):\n",
    "                new_path = path + [next_node]\n",
    "                new_score = score + weight\n",
    "                next_cand.append((new_path, new_score))\n",
    "        \n",
    "        if not next_cand:\n",
    "            break\n",
    "        \n",
    "        # Keep top-k candidates\n",
    "        candidates = sorted(next_cand, key=lambda x: x[1], reverse=True)[:beam]\n",
    "        \n",
    "        # Stop if all candidates reached stopping condition\n",
    "        if all(p[-1] == end_id or len(p) > length_threshold for p, _ in candidates):\n",
    "            break\n",
    "    \n",
    "    # Convert to token sequences\n",
    "    results = []\n",
    "    for path, score in candidates:\n",
    "        # Extract tokens, excluding START and END\n",
    "        tokens = []\n",
    "        for node_id in path:\n",
    "            if node_id not in (start_id, end_id):\n",
    "                token = new_to_tok[node_id]\n",
    "                tokens.append(token)\n",
    "        results.append((tokens, score))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def greedy_reconstruct(pruned_adj: torch.Tensor, new_to_tok: Dict[int, str], start_id: int, end_id: int,\n",
    "                       total_tokens: int, k_threshold: float = 1.5) -> Tuple[List[str], int]:\n",
    "    \"\"\"\n",
    "    Greedy reconstruction: always pick highest weight edge.\n",
    "    Allows duplicates, stops when length > k*total_tokens or END reached.\n",
    "    \n",
    "    Returns:\n",
    "        (token_sequence, path_length)\n",
    "    \"\"\"\n",
    "    # Build adjacency dict\n",
    "    indices = pruned_adj.coalesce().indices()\n",
    "    values = pruned_adj.coalesce().values()\n",
    "    \n",
    "    adj_dict = defaultdict(list)\n",
    "    for i in range(indices.shape[1]):\n",
    "        u, v, w = indices[0, i].item(), indices[1, i].item(), values[i].item()\n",
    "        adj_dict[u].append((v, w))\n",
    "    \n",
    "    length_threshold = int(k_threshold * total_tokens)\n",
    "    \n",
    "    path = [start_id]\n",
    "    current = start_id\n",
    "    \n",
    "    for step in range(length_threshold * 2):  # Hard limit\n",
    "        # Stop if reached END or exceeded threshold\n",
    "        if current == end_id or len(path) > length_threshold:\n",
    "            break\n",
    "        \n",
    "        neighbors = adj_dict.get(current, [])\n",
    "        if not neighbors:\n",
    "            break\n",
    "        \n",
    "        # Pick highest weight neighbor (allow revisiting)\n",
    "        next_node, weight = max(neighbors, key=lambda x: x[1])\n",
    "        path.append(next_node)\n",
    "        current = next_node\n",
    "    \n",
    "    # Extract tokens\n",
    "    tokens = [new_to_tok[i] for i in path if i not in (start_id, end_id)]\n",
    "    return tokens, len(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ **uses TF weights** – **never touches BSS lattice**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.  Demo (same corpus, new pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 1: Ingest Corpus ===\n",
      "tensor(indices=tensor([[ 0,  0,  0,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12,\n",
      "                        13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 24, 25, 26,\n",
      "                        27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41,\n",
      "                        42, 43, 44, 45, 46, 47, 49, 50],\n",
      "                       [ 3, 20, 37,  4,  2,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
      "                        14, 15, 16, 18,  1, 21, 19, 22, 23, 24, 25, 40, 26, 27,\n",
      "                        28, 29, 30, 31, 32, 33, 35,  1, 38, 36, 39, 23, 41, 42,\n",
      "                        43, 44, 45, 46, 47, 48, 50,  1]]),\n",
      "       values=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1.,\n",
      "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "                      1., 1., 1., 1., 1., 1., 1., 1.]),\n",
      "       size=(51, 51), nnz=50, layout=torch.sparse_coo)\n",
      "{'<START>': 0, '<END>': 1, '人': 2, '<START>人': 3, '人工': 4, '人工智': 5, '智': 6, '智能': 7, '智能正': 8, '正': 9, '正在': 10, '正在改': 11, '改': 12, '改变': 13, '改变世': 14, '世': 15, '世界': 16, '界': 17, '界<END>': 18, '机': 19, '<START>机': 20, '机器': 21, '机器学': 22, '学': 23, '学习': 24, '学习让': 25, '让': 26, '让代': 27, '让代码': 28, '码': 29, '码更': 30, '码更聪': 31, '聪': 32, '聪明': 33, '明': 34, '明<END>': 35, '深': 36, '<START>深': 37, '深度': 38, '深度学': 39, '学习是': 40, '是': 41, '是未': 42, '是未来': 43, '来': 44, '来的': 45, '来的钥': 46, '钥': 47, '钥匙': 48, '匙': 49, '匙<END>': 50}\n",
      "{3641065299: {'tokens': ['<START>'], 'hll_pair': (851, 22)}, 3355658904: {'tokens': ['<END>'], 'hll_pair': (664, 22)}, 1180368444: {'tokens': ['人'], 'hll_pair': (572, 21)}, 1538322295: {'tokens': ['<START>人'], 'hll_pair': (887, 21)}, 4138951398: {'tokens': ['人工'], 'hll_pair': (742, 22)}, 2337013181: {'tokens': ['人工智'], 'hll_pair': (445, 22)}, 3298667204: {'tokens': ['智'], 'hll_pair': (708, 22)}, 2523237615: {'tokens': ['智能'], 'hll_pair': (239, 22)}, 3049440050: {'tokens': ['智能正'], 'hll_pair': (818, 22)}, 259353663: {'tokens': ['正'], 'hll_pair': (63, 18)}, 3888316716: {'tokens': ['正在'], 'hll_pair': (300, 22)}, 680141127: {'tokens': ['正在改'], 'hll_pair': (327, 20)}, 4230229906: {'tokens': ['改'], 'hll_pair': (914, 22)}, 380206337: {'tokens': ['改变'], 'hll_pair': (257, 19)}, 575713220: {'tokens': ['改变世'], 'hll_pair': (964, 20)}, 3969544043: {'tokens': ['世'], 'hll_pair': (875, 22)}, 3230053120: {'tokens': ['世界'], 'hll_pair': (768, 22)}, 2881866711: {'tokens': ['界'], 'hll_pair': (983, 22)}, 143818949: {'tokens': ['界<END>'], 'hll_pair': (197, 18)}, 598160102: {'tokens': ['机'], 'hll_pair': (742, 20)}, 947949854: {'tokens': ['<START>机'], 'hll_pair': (286, 20)}, 1716212061: {'tokens': ['机器'], 'hll_pair': (349, 21)}, 4278108330: {'tokens': ['机器学'], 'hll_pair': (170, 22)}, 1710389796: {'tokens': ['学'], 'hll_pair': (548, 21)}, 1324687574: {'tokens': ['学习'], 'hll_pair': (214, 21)}, 2860377765: {'tokens': ['学习让'], 'hll_pair': (677, 22)}, 377206236: {'tokens': ['让'], 'hll_pair': (476, 19)}, 1442903220: {'tokens': ['让代'], 'hll_pair': (180, 21)}, 2192006461: {'tokens': ['让代码'], 'hll_pair': (317, 22)}, 801082575: {'tokens': ['码'], 'hll_pair': (207, 20)}, 1188363316: {'tokens': ['码更'], 'hll_pair': (52, 21)}, 2415014886: {'tokens': ['码更聪'], 'hll_pair': (998, 22)}, 3222517660: {'tokens': ['聪'], 'hll_pair': (924, 22)}, 3690162293: {'tokens': ['聪明'], 'hll_pair': (117, 22)}, 3863466255: {'tokens': ['明'], 'hll_pair': (271, 22)}, 2815244086: {'tokens': ['明<END>'], 'hll_pair': (822, 22)}, 564943433: {'tokens': ['深'], 'hll_pair': (585, 20)}, 79542871: {'tokens': ['<START>深'], 'hll_pair': (599, 17)}, 595571098: {'tokens': ['深度'], 'hll_pair': (410, 20)}, 3033540473: {'tokens': ['深度学'], 'hll_pair': (889, 22)}, 1277647898: {'tokens': ['学习是'], 'hll_pair': (26, 21)}, 174107791: {'tokens': ['是'], 'hll_pair': (143, 18)}, 1809419365: {'tokens': ['是未'], 'hll_pair': (101, 21)}, 2611627597: {'tokens': ['是未来'], 'hll_pair': (589, 22)}, 4155548605: {'tokens': ['来'], 'hll_pair': (957, 22)}, 1743404924: {'tokens': ['来的'], 'hll_pair': (892, 21)}, 2051130118: {'tokens': ['来的钥'], 'hll_pair': (774, 21)}, 3787627444: {'tokens': ['钥'], 'hll_pair': (948, 22)}, 45510325: {'tokens': ['钥匙'], 'hll_pair': (693, 16)}, 2956026251: {'tokens': ['匙'], 'hll_pair': (395, 22)}, 1078639039: {'tokens': ['匙<END>'], 'hll_pair': (447, 21)}}\n",
      "Total tokens in AM: 51\n",
      "Total edges: 50\n",
      "\n",
      "=== Step 2: Build HLLSet ===\n",
      "HLLSet size: 51 unique (reg, run) pairs\n",
      "\n",
      "=== Step 3: Disambiguate Tokens ===\n",
      "Disambiguated tokens: 16\n",
      "\n",
      "=== Step 4: Prune Adjacency Matrix ===\n",
      "Pruned edges: 0\n",
      "Pruned nodes: 16\n",
      "\n",
      "=== Step 5A: Greedy Reconstruction ===\n",
      "Path length: 1\n",
      "1-grams only: \n",
      "Full sequence: \n",
      "\n",
      "=== Step 5B: Beam Search Reconstruction ===\n",
      "\n",
      "Candidate 1 (score=0.00):\n",
      "  1-grams: \n",
      "  Length: 0 tokens\n",
      "  Sample: \n"
     ]
    }
   ],
   "source": [
    "CORPUS = [\n",
    "    \"人工智能正在改变世界\",\n",
    "    \"机器学习让代码更聪明\",\n",
    "    \"深度学习是未来的钥匙\"\n",
    "]\n",
    "\n",
    "print(\"=== Step 1: Ingest Corpus ===\")\n",
    "adj, tok_id, lut = ingest_corpus(CORPUS)\n",
    "\n",
    "print(adj)\n",
    "print(tok_id)\n",
    "print(lut.table)\n",
    "\n",
    "print(f\"Total tokens in AM: {len(tok_id)}\")\n",
    "print(f\"Total edges: {adj._nnz()}\")\n",
    "\n",
    "print(\"\\n=== Step 2: Build HLLSet ===\")\n",
    "hllset = build_hllset(lut)\n",
    "print(f\"HLLSet size: {len(hllset)} unique (reg, run) pairs\")\n",
    "\n",
    "print(\"\\n=== Step 3: Disambiguate Tokens ===\")\n",
    "disambiguated = unambiguate_tokens(hllset, lut, tok_id)\n",
    "print(f\"Disambiguated tokens: {len(disambiguated)}\")\n",
    "\n",
    "print(\"\\n=== Step 4: Prune Adjacency Matrix ===\")\n",
    "pruned_adj, old_to_new, new_to_tok = prune_adjacency(adj, disambiguated, tok_id)\n",
    "start_id = old_to_new[tok_id[\"<START>\"]]\n",
    "end_id = old_to_new[tok_id[\"<END>\"]]\n",
    "print(f\"Pruned edges: {pruned_adj._nnz()}\")\n",
    "print(f\"Pruned nodes: {len(new_to_tok)}\")\n",
    "\n",
    "print(\"\\n=== Step 5A: Greedy Reconstruction ===\")\n",
    "tokens_greedy, path_len = greedy_reconstruct(\n",
    "    pruned_adj, new_to_tok, start_id, end_id, \n",
    "    total_tokens=len(disambiguated),\n",
    "    k_threshold=1.5\n",
    ")\n",
    "print(f\"Path length: {path_len}\")\n",
    "print(f\"1-grams only: {''.join([t for t in tokens_greedy if len(t) == 1])}\")\n",
    "print(f\"Full sequence: {' '.join(tokens_greedy[:50])}\")  # First 50 tokens\n",
    "\n",
    "print(\"\\n=== Step 5B: Beam Search Reconstruction ===\")\n",
    "candidates = reconstruct_order(\n",
    "    pruned_adj, new_to_tok, start_id, end_id,\n",
    "    total_tokens=len(disambiguated),\n",
    "    k_threshold=1.5,\n",
    "    beam=3\n",
    ")\n",
    "\n",
    "for i, (tokens, score) in enumerate(candidates[:3], 1):\n",
    "    text_1g = ''.join([t for t in tokens if len(t) == 1])\n",
    "    print(f\"\\nCandidate {i} (score={score:.2f}):\")\n",
    "    print(f\"  1-grams: {text_1g[:100]}\")  # First 100 chars\n",
    "    print(f\"  Length: {len(tokens)} tokens\")\n",
    "    print(f\"  Sample: {' '.join(tokens[:20])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hllset-swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
