{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjacency Matrix (AM)\n",
    "\n",
    ">hash-addressable, dynamically-growing, PyTorch-only implementation\n",
    "\n",
    "```math\n",
    "r(hᵢ, hⱼ) ← v  (\\text{ overwrite if exists, insert if new})\n",
    "```\n",
    "\n",
    "for an arbitrary number of tokens whose only identifier is the hash value h = hash(tᵢ).\n",
    "\n",
    "\n",
    "No Python loops touch the tensor itself; all lookups and updates are O(1) and executed with one tensor op.\n",
    "\n",
    "## Data layout\n",
    "\n",
    "We keep a single 2-D tensor R of shape (capacity, capacity)\n",
    "row/col indices are internal integer ids (0,1,2,…) that we map to/from the hash values with two dictionaries:\n",
    "\n",
    "```text\n",
    "hash → idx   : self.h2i\n",
    "idx  → hash  : self.i2h\n",
    "```\n",
    "\n",
    "capacity grows automatically in powers of two (doubling when >75 % full) and the old contents are copied into the new larger tensor with a single index_copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Check available GPUs\n",
    "print(\"Available GPUs:\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    props = torch.cuda.get_device_properties(i)\n",
    "    print(f\"  GPU {i}: {props.name}\")\n",
    "    print(f\"    Compute Capability: sm_{props.major}{props.minor}\")\n",
    "    print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# Select RTX 3060 (adjust index based on output above)\n",
    "# Option A: Hide Quadro, only show RTX\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # Assuming RTX is at index 1\n",
    "DEVICE = torch.device(\"cuda:0\")  # Now index 0 refers to the RTX\n",
    "\n",
    "# Option B: Explicitly select by index (if you don't set CUDA_VISIBLE_DEVICES)\n",
    "# DEVICE = torch.device(\"cuda:1\")  # Direct access to RTX at original index 1\n",
    "\n",
    "print(f\"\\nUsing device: {DEVICE}\")\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(DEVICE)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(DEVICE).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class HashRelationTensor:\n",
    "    \"\"\"\n",
    "    Binary relation tensor r(h_i, h_j) = v  addressed only by hashes.\n",
    "    Shape grows automatically; all updates are one tensor op.\n",
    "    \"\"\"\n",
    "    def __init__(self, init_cap=16, dtype=torch.float32, device='cpu'):\n",
    "        self.device   = device\n",
    "        self.dtype    = dtype\n",
    "        self.capacity = init_cap\n",
    "        self.R        = torch.zeros(init_cap, init_cap, dtype=dtype, device=device)\n",
    "\n",
    "        self.h2i      = {}          # hash -> internal index\n",
    "        self.i2h      = {}          # internal index -> hash\n",
    "        self.next_idx = 0           # first free internal index\n",
    "\n",
    "    # ---------- public ----------\n",
    "    def update(self, h_i: int, h_j: int, value):\n",
    "        \"\"\"h_i, h_j are hash values (python ints)\"\"\"\n",
    "        # obtain (or create) internal indices\n",
    "        i = self._index_for_hash(h_i)\n",
    "        j = self._index_for_hash(h_j)\n",
    "        # single in-place tensor write\n",
    "        self.R[i, j] = value\n",
    "\n",
    "    def get(self, h_i: int, h_j: int):\n",
    "        if h_i not in self.h2i or h_j not in self.h2i:\n",
    "            raise KeyError(\"Relation not stored\")\n",
    "        return self.R[self.h2i[h_i], self.h2i[h_j]]\n",
    "\n",
    "    def get_dense(self) -> torch.Tensor:\n",
    "        \"\"\"Return the dense sub-matrix actually in use (shape next_idx × next_idx)\"\"\"\n",
    "        return self.R[:self.next_idx, :self.next_idx].clone()\n",
    "    \n",
    "    def prune(self, keep: List[int]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Drop every token whose hash is NOT in `keep`.\n",
    "        Rebuild h2i/i2h and shrink self.R to |keep|×|keep| in-place.\n",
    "        Returns the dense pruned adjacency matrix.\n",
    "        \"\"\"\n",
    "        keep_set = set(keep)\n",
    "        # 1. build new contiguous index mapping\n",
    "        new_h2i = {h: idx for idx, h in enumerate(h for h in keep if h in self.h2i)}\n",
    "        n = len(new_h2i)\n",
    "        if n == 0:                       # nothing left\n",
    "            self.h2i.clear()\n",
    "            self.i2h.clear()\n",
    "            self.next_idx = 0\n",
    "            self.capacity = 0\n",
    "            self.R = torch.empty(0, 0, dtype=self.dtype, device=self.device)\n",
    "            return self.R\n",
    "\n",
    "        # 2. allocate compact matrix\n",
    "        new_R = torch.zeros(n, n, dtype=self.dtype, device=self.device)\n",
    "\n",
    "        # 3. copy old values into new positions (vectorised)\n",
    "        old_rows, old_cols = [], []\n",
    "        new_rows, new_cols = [], []\n",
    "        vals = []\n",
    "        for h_row in new_h2i:\n",
    "            for h_col in new_h2i:\n",
    "                old_rows.append(self.h2i[h_row])\n",
    "                old_cols.append(self.h2i[h_col])\n",
    "                new_rows.append(new_h2i[h_row])   # ✅ use NEW indices\n",
    "                new_cols.append(new_h2i[h_col])\n",
    "                vals.append(self.R[self.h2i[h_row], self.h2i[h_col]])\n",
    "\n",
    "        if vals:\n",
    "            new_R[torch.tensor(new_rows, device=self.device),\n",
    "                torch.tensor(new_cols, device=self.device)] = torch.tensor(vals, device=self.device)\n",
    "\n",
    "        # 4. overwrite internal state\n",
    "        self.h2i = new_h2i\n",
    "        self.i2h = {idx: h for h, idx in new_h2i.items()}\n",
    "        self.R = new_R\n",
    "        self.capacity = n\n",
    "        self.next_idx = n\n",
    "        return new_R\n",
    "    \n",
    "    # ====== history (incoming edges) ======\n",
    "    def get_history(self, h: int, horizon: int = 1) -> Dict[int, float]:\n",
    "        \"\"\"Return {hash_of_past_token : edge_strength} within horizon steps.\"\"\"\n",
    "        if h not in self.h2i:\n",
    "            return {}\n",
    "        idx = self.h2i[h]\n",
    "        # 1-step predecessors\n",
    "        mask = self.R[:, idx] != 0          # (capacity,)\n",
    "        preds = mask.nonzero(as_tuple=False).squeeze(1)  # (<=capacity,)\n",
    "        strengths = self.R[preds, idx]      # (<=capacity,)\n",
    "        # truncate to horizon closest (here 1 step – see multi-step below)\n",
    "        topk = min(horizon, len(preds))\n",
    "        if topk == 0:\n",
    "            return {}\n",
    "        top_vals, top_idx = torch.topk(strengths, k=topk)\n",
    "        return {self.i2h[int(i)]: float(v) for i, v in zip(preds[top_idx], top_vals)}\n",
    "\n",
    "    def get_history_batch(self, hs: List[int], horizon: int = 1) -> List[Dict[int, float]]:\n",
    "        return [self.get_history(h, horizon) for h in hs]\n",
    "\n",
    "    # ====== future (outgoing edges) ======\n",
    "    def get_future(self, h: int, horizon: int = 1) -> Dict[int, float]:\n",
    "        \"\"\"Return {hash_of_future_token : edge_strength} within horizon steps.\"\"\"\n",
    "        if h not in self.h2i:\n",
    "            return {}\n",
    "        idx = self.h2i[h]\n",
    "        mask = self.R[idx, :] != 0\n",
    "        succs = mask.nonzero(as_tuple=False).squeeze(1)\n",
    "        strengths = self.R[idx, succs]\n",
    "        topk = min(horizon, len(succs))\n",
    "        if topk == 0:\n",
    "            return {}\n",
    "        top_vals, top_idx = torch.topk(strengths, k=topk)\n",
    "        return {self.i2h[int(i)]: float(v) for i, v in zip(succs[top_idx], top_vals)}\n",
    "\n",
    "    def get_future_batch(self, hs: List[int], horizon: int = 1) -> List[Dict[int, float]]:\n",
    "        return [self.get_future(h, horizon) for h in hs]\n",
    "\n",
    "    # ====== projection (dense sub-matrix for a given token set) ======\n",
    "    def project(self, hs: List[int]) -> torch.Tensor:\n",
    "        \"\"\"Return dense |hs|×|hs| relation matrix for the supplied hashes.\"\"\"\n",
    "        idx = [self.h2i[h] for h in hs if h in self.h2i]\n",
    "        if not idx:\n",
    "            return torch.empty(0, 0, dtype=self.dtype, device=self.device)\n",
    "        idx_t = torch.tensor(idx, device=self.device)\n",
    "        return self.R[idx_t][:, idx_t]   # elegant batched indexing\n",
    "\n",
    "    # ---------- internal ----------\n",
    "    def _index_for_hash(self, h):\n",
    "        \"\"\"Return internal index for hash h, growing tensor if necessary.\"\"\"\n",
    "        if h in self.h2i:\n",
    "            return self.h2i[h]\n",
    "        # new hash → assign next internal index\n",
    "        if self.next_idx >= self.capacity:\n",
    "            self._grow()\n",
    "        idx = self.next_idx\n",
    "        self.h2i[h] = idx\n",
    "        self.i2h[idx] = h\n",
    "        self.next_idx += 1\n",
    "        return idx\n",
    "\n",
    "    def _grow(self):\n",
    "        old_cap = self.capacity\n",
    "        new_cap = old_cap * 2\n",
    "        new_R = torch.zeros(new_cap, new_cap, dtype=self.dtype, device=self.device)\n",
    "        # copy old block in one tensor op\n",
    "        new_R[:old_cap, :old_cap] = self.R\n",
    "        self.R = new_R\n",
    "        self.capacity = new_cap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9000, device='cuda:0')\n",
      "tensor([[0.0000, 0.9000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.0000]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "dev = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "rel = HashRelationTensor(device=dev)\n",
    "\n",
    "tok_hashes = [hash(t) for t in [\"cat\", \"dog\", \"bird\"]]\n",
    "\n",
    "rel.update(tok_hashes[0], tok_hashes[1], 0.8)   # r(cat,dog)=0.8\n",
    "rel.update(tok_hashes[1], tok_hashes[2], 0.5)   # r(dog,bird)=0.5\n",
    "rel.update(tok_hashes[0], tok_hashes[1], 0.9)   # overwrite with 0.9\n",
    "\n",
    "print(rel.get(tok_hashes[0], tok_hashes[1]))    # tensor(0.9, device='cuda:0')\n",
    "print(rel.get_dense())                          # 3×3 dense sub-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2053597437022370697: 0.699999988079071, 895199035842044268: 0.5}\n",
      "{5354350517194846248: 0.8999999761581421, -732056921184991155: 0.5}\n",
      "tensor([[0.0000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.7000],\n",
      "        [0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "rel = HashRelationTensor()\n",
    "\n",
    "# build a tiny time graph\n",
    "t = [hash(w) for w in [\"t0\", \"t1\", \"t2\", \"t3\"]]\n",
    "rel.update(t[0], t[1], 0.9)   # t0 → t1\n",
    "rel.update(t[1], t[2], 0.8)   # t1 → t2\n",
    "rel.update(t[2], t[3], 0.7)   # t2 → t3\n",
    "rel.update(t[0], t[3], 0.5)   # t0 → t3  (skip connection)\n",
    "\n",
    "print(rel.get_history(t[3], horizon=2))\n",
    "# {hash(t2): 0.7, hash(t0): 0.5}   two strongest predecessors\n",
    "\n",
    "print(rel.get_future(t[0], horizon=2))\n",
    "# {hash(t1): 0.9, hash(t3): 0.5}   two strongest successors\n",
    "\n",
    "print(rel.project([t[0], t[2], t[3]]))\n",
    "# 3×3 dense tensor of relations among exactly these three tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning Demo\n",
    "\n",
    "### Complexity\n",
    "\n",
    "Time: O(|keep|²) – once.\n",
    "Memory: the old matrix is discarded immediately; only the pruned one remains.\n",
    "All subsequent operations (update, get_history, project, …) run on the smaller matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before prune torch.Size([16, 16])\n",
      "after prune torch.Size([3, 3])\n",
      "{6799080192888523577: 0, 7451729274442981443: 1, 4782691667229046252: 2}\n",
      "{6799080192888523577: 4.0}\n"
     ]
    }
   ],
   "source": [
    "rel = HashRelationTensor()\n",
    "\n",
    "# build a graph with 4 tokens\n",
    "t = [hash(w) for w in [\"A\", \"B\", \"C\", \"D\"]]\n",
    "rel.update(t[0], t[1], 1.0)\n",
    "rel.update(t[1], t[2], 2.0)\n",
    "rel.update(t[2], t[3], 3.0)\n",
    "rel.update(t[0], t[3], 4.0)\n",
    "\n",
    "print(\"before prune\", rel.R.shape)      # 4×4\n",
    "\n",
    "pruned = rel.prune([t[0], t[2], t[3]])  # drop B\n",
    "print(\"after prune\", pruned.shape)      # 3×3\n",
    "print(rel.h2i)                          # {hash_A:0, hash_C:1, hash_D:2}\n",
    "\n",
    "# history/future now work only on the pruned set\n",
    "print(rel.get_history(t[3]))            # {hash_A:4.0, hash_C:3.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity\n",
    "\n",
    "- update/get: O(1) average time, one tensor write.\n",
    "- memory: O(N²) where N = number of distinct hashes (only the actually used N×N block is returned by get_dense()).\n",
    "- growth: amortised O(1); when capacity doubles we pay one index_copy of the existing N×N block.\n",
    "\n",
    "## Extensions\n",
    "\n",
    "1. Symmetric relation: mirror each write R[j,i] = R[i,j].\n",
    "2. Directed vs undirected: store only upper triangle and use a triu_indices view.\n",
    "3. Mini-batch updates: vectorise with index_put_ and two integer tensors rows, cols."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hllset-swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
