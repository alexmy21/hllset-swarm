{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q1: Can we use context to separate elements into strata?**\n",
    "\n",
    "**Absolutely yes.** This is essentially **contextual clustering via hash collision patterns**. The key insight is that while œï is many-to-one at the token level, the *patterns* of collisions reveal contextual relationships.\n",
    "\n",
    "### **Formal Mechanism**:\n",
    "\n",
    "Given:\n",
    "- Tokens `t‚ÇÅ, t‚ÇÇ` that collide to the same bit `b`\n",
    "- Their respective contextual HLLSets: `C(t‚ÇÅ)`, `C(t‚ÇÇ)` (sets of tokens that co-occur with each)\n",
    "\n",
    "The **contextual divergence** grows as:\n",
    "```math\n",
    "\\text{Divergence}(t‚ÇÅ, t‚ÇÇ) = 1 - \\text{Jaccard}(C(t‚ÇÅ), C(t‚ÇÇ))\n",
    "```\n",
    "\n",
    "Even when `œï(t‚ÇÅ) = œï(t‚ÇÇ)`, if `C(t‚ÇÅ)` and `C(t‚ÇÇ)` are contextually separated, their *extended contexts* will activate different **patterns of bits**.\n",
    "\n",
    "## **Q2: Does this make sense?**\n",
    "\n",
    "**Profoundly yes.** This addresses the fundamental \"hash collision curse\" of probabilistic data structures by turning it into a **feature** rather than a bug.\n",
    "\n",
    "### **Why it works**:\n",
    "\n",
    "1. **Natural Language Contexts**: In real data, \"bank\" (financial) and \"bank\" (river) appear with different surrounding words\n",
    "2. **Temporal Contexts**: \"apple\" (fruit) vs \"Apple\" (company) appear in different time periods/news contexts\n",
    "3. **Domain Contexts**: \"mouse\" (animal) vs \"mouse\" (computer) appear in different document collections\n",
    "\n",
    "The collision patterns create a **contextual fingerprint** that transcends individual bit collisions.\n",
    "\n",
    "## **Q3: Formalization Framework**\n",
    "\n",
    "Here's a complete formalization using the existing HLLSet category theory:\n",
    "\n",
    "### **3.1 Contextual Strata Definition**\n",
    "\n",
    "Let `ùíØ` be the token universe. Define **contextual equivalence**:\n",
    "\n",
    "```math\n",
    "t‚ÇÅ ‚àº_C t‚ÇÇ \\iff \\text{BSS}_œÑ(C(t‚ÇÅ) ‚Üí C(t‚ÇÇ)) ‚â• Œ∏ \\quad \\text{and} \\quad \\text{BSS}_œÅ(C(t‚ÇÅ) ‚Üí C(t‚ÇÇ)) < Œ¥\n",
    "```\n",
    "\n",
    "Where:\n",
    "\n",
    "- `C(t)` = HLLSet of tokens co-occurring with `t` within window `w`\n",
    "- `Œ∏` = high inclusion threshold (e.g., 0.8)\n",
    "- `Œ¥` = low exclusion threshold (e.g., 0.1)\n",
    "\n",
    "**Strata** are equivalence classes: $S_i = [t]_{‚àº_C}$\n",
    "\n",
    "### **3.2 Stratified HLLSet Basis**\n",
    "\n",
    "For each stratum `S_i`, create **basis HLLSet**:\n",
    "\n",
    "```math\n",
    "B_i = \\bigcup_{t ‚àà S_i} œï(t)\n",
    "```\n",
    "\n",
    "These basis sets have the property:\n",
    "\n",
    "- **Within-stratum cohesion**: Elements in same stratum have similar contextual footprints\n",
    "- **Between-stratum separation**: Different strata activate different bit patterns\n",
    "\n",
    "### **3.3 Optimal Representation Theorem**\n",
    "\n",
    "**Theorem**: Any HLLSet `A` can be approximated as:\n",
    "\n",
    "```math\n",
    "A ‚âà \\bigcup_{i ‚àà I} Œ±_i B_i \\quad \\text{where } Œ±_i ‚àà \\{0,1\\}\n",
    "```\n",
    "\n",
    "**Minimal Overlap Objective**:\n",
    "\n",
    "```math\n",
    "\\min_I \\left[ \\sum_{i‚ààI} |B_i| - \\left| \\bigcap_{i‚ààI} B_i \\right| \\right]\n",
    "\\text{ subject to } \\text{Coverage}(A, \\bigcup_{i‚ààI} B_i) ‚â• Œ≥\n",
    "```\n",
    "\n",
    "### **3.4 Implementation Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HLLSet kernel from: None\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Tuple, Set, Optional\n",
    "from hllset_swarm.hllset_wrapper import HllSet, HllHashInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StratifiedHLLBasis:\n",
    "    \"\"\"Build contextual strata as basis for HLLSet representation\"\"\"\n",
    "    \n",
    "    def __init__(self, corpus: List[List[str]], window_size: int = 5):\n",
    "        self.corpus = corpus\n",
    "        self.window_size = window_size\n",
    "        self.token_contexts = {}  # t -> HLLSet of co-occurring tokens\n",
    "        self.strata = []\n",
    "        \n",
    "    def build_contexts(self):\n",
    "        \"\"\"Build contextual HLLSets for each token\"\"\"\n",
    "        for document in self.corpus:\n",
    "            for i, token in enumerate(document):\n",
    "                # Get context window\n",
    "                start = max(0, i - self.window_size)\n",
    "                end = min(len(document), i + self.window_size + 1)\n",
    "                context_tokens = document[start:end]\n",
    "                context_tokens.remove(token)  # Exclude self\n",
    "                \n",
    "                # Build or update context HLLSet\n",
    "                if token not in self.token_contexts:\n",
    "                    self.token_contexts[token] = HLLSet()\n",
    "                for ctx_token in context_tokens:\n",
    "                    self.token_contexts[token].add(ctx_token)\n",
    "    \n",
    "    def cluster_into_strata(self, theta: float = 0.8, delta: float = 0.1):\n",
    "        \"\"\"Cluster tokens by contextual similarity\"\"\"\n",
    "        tokens = list(self.token_contexts.keys())\n",
    "        visited = set()\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token in visited:\n",
    "                continue\n",
    "                \n",
    "            # Start new stratum with this token\n",
    "            stratum = {token}\n",
    "            \n",
    "            # Find all contextually similar tokens\n",
    "            for other in tokens:\n",
    "                if other in visited:\n",
    "                    continue\n",
    "                    \n",
    "                # Check contextual equivalence\n",
    "                ctx1 = self.token_contexts[token]\n",
    "                ctx2 = self.token_contexts[other]\n",
    "                \n",
    "                bss_tau = ctx1.bss_tau(ctx2)\n",
    "                bss_rho = ctx1.bss_rho(ctx2)\n",
    "                \n",
    "                if bss_tau >= theta and bss_rho < delta:\n",
    "                    stratum.add(other)\n",
    "                    visited.add(other)\n",
    "            \n",
    "            self.strata.append(stratum)\n",
    "            visited.add(token)\n",
    "    \n",
    "    def build_basis_HLLSets(self) -> Dict[int, HllSet]:\n",
    "        \"\"\"Create basis HLLSet for each stratum\"\"\"\n",
    "        basis = {}\n",
    "        for i, stratum in enumerate(self.strata):\n",
    "            basis_set = HLLSet()\n",
    "            for token in stratum:\n",
    "                # Add token to basis set\n",
    "                basis_set.add(token)\n",
    "            basis[i] = basis_set\n",
    "        \n",
    "        return basis\n",
    "    \n",
    "    def represent_HLLSet(self, target: HllSet, \n",
    "                        method: str = 'minimal_cover') -> Dict:\n",
    "        \"\"\"\n",
    "        Represent target HLLSet using basis sets\n",
    "        \n",
    "        Args:\n",
    "            target: HLLSet to represent\n",
    "            method: 'minimal_cover', 'optimal_composition', or 'sparse_coding'\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with representation and metrics\n",
    "        \"\"\"\n",
    "        basis = self.build_basis_HLLSets()\n",
    "        \n",
    "        if method == 'minimal_cover':\n",
    "            return self._minimal_cover(target, basis)\n",
    "        elif method == 'optimal_composition':\n",
    "            return self._optimal_composition(target, basis)\n",
    "        elif method == 'sparse_coding':\n",
    "            return self._sparse_coding(target, basis)\n",
    "    \n",
    "    def _minimal_cover(self, target: HllSet, basis: Dict[int, HllSet]) -> Dict:\n",
    "        \"\"\"Find minimal set of basis HLLSets covering target\"\"\"\n",
    "        # Use greedy set cover algorithm\n",
    "        uncovered = set(target.get_set_bits())\n",
    "        selected_indices = []\n",
    "        \n",
    "        while uncovered and basis:\n",
    "            # Find basis set covering most uncovered bits\n",
    "            best_idx = None\n",
    "            best_coverage = 0\n",
    "            \n",
    "            for idx, basis_set in basis.items():\n",
    "                coverage = len(uncovered.intersection(basis_set.get_set_bits()))\n",
    "                if coverage > best_coverage:\n",
    "                    best_coverage = coverage\n",
    "                    best_idx = idx\n",
    "            \n",
    "            if best_idx is None:\n",
    "                break\n",
    "                \n",
    "            # Add to cover\n",
    "            selected_indices.append(best_idx)\n",
    "            uncovered -= set(basis[best_idx].get_set_bits())\n",
    "            del basis[best_idx]\n",
    "        \n",
    "        # Build representation\n",
    "        representation = HLLSet()\n",
    "        for idx in selected_indices:\n",
    "            representation = representation.union(self.basis[idx])\n",
    "        \n",
    "        return {\n",
    "            'basis_indices': selected_indices,\n",
    "            'representation': representation,\n",
    "            'coverage': 1 - len(uncovered) / len(target.get_set_bits()),\n",
    "            'compression_ratio': len(selected_indices) / len(self.strata)\n",
    "        }\n",
    "    \n",
    "    def _optimal_composition(self, target: HllSet, basis: Dict[int, HllSet]) -> Dict:\n",
    "        \"\"\"Find optimal linear combination of basis sets\"\"\"\n",
    "        # Formulate as integer programming problem\n",
    "        # Minimize: Œ£ x_i + Œª * Œ£ |w_i - 0.5|\n",
    "        # Subject to: Coverage(Œ£ w_i * B_i, Target) ‚â• Œ≥\n",
    "        \n",
    "        n = len(basis)\n",
    "        coverage_matrix = np.zeros((target.m, n))\n",
    "        \n",
    "        # Build coverage matrix\n",
    "        for j, (idx, basis_set) in enumerate(basis.items()):\n",
    "            bits = basis_set.get_set_bits()\n",
    "            for bit in bits:\n",
    "                coverage_matrix[bit, j] = 1\n",
    "        \n",
    "        target_vector = np.array([1 if bit in target.get_set_bits() else 0 \n",
    "                                 for bit in range(target.m)])\n",
    "        \n",
    "        # Solve: minimize ||Ax - b|| + Œª||x||‚ÇÅ\n",
    "        # Using Lasso regression for sparse composition\n",
    "        from sklearn.linear_model import Lasso\n",
    "        \n",
    "        model = Lasso(alpha=0.1)\n",
    "        model.fit(coverage_matrix, target_vector)\n",
    "        \n",
    "        weights = model.coef_\n",
    "        selected = np.where(np.abs(weights) > 0.01)[0]\n",
    "        \n",
    "        # Build weighted representation\n",
    "        representation = HLLSet()\n",
    "        for idx, weight in enumerate(weights):\n",
    "            if abs(weight) > 0.01:\n",
    "                # Threshold to include if weight > threshold\n",
    "                representation = representation.union(basis[idx])\n",
    "        \n",
    "        return {\n",
    "            'weights': weights,\n",
    "            'selected_indices': selected.tolist(),\n",
    "            'representation': representation,\n",
    "            'sparsity': len(selected) / n,\n",
    "            'reconstruction_error': np.mean((model.predict(coverage_matrix) - target_vector) ** 2)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_stratified_representation():\n",
    "    \"\"\"Test the stratified representation hypothesis\"\"\"\n",
    "    \n",
    "    # 1. Generate synthetic data with clear contextual clusters\n",
    "    data = generate_contextual_clusters(\n",
    "        n_clusters=10,\n",
    "        tokens_per_cluster=100,\n",
    "        cross_cluster_noise=0.1\n",
    "    )\n",
    "    \n",
    "    # 2. Build stratified basis\n",
    "    stratifier = StratifiedHLLBasis(data)\n",
    "    stratifier.build_contexts()\n",
    "    stratifier.cluster_into_strata()\n",
    "    basis = stratifier.build_basis_HLLSets()\n",
    "    \n",
    "    # 3. Test representation accuracy\n",
    "    test_sets = generate_test_HLLSets(data)\n",
    "    \n",
    "    results = []\n",
    "    for test_set in test_sets:\n",
    "        representation = stratifier.represent_HLLSet(test_set)\n",
    "        \n",
    "        # Metrics\n",
    "        accuracy = test_set.jaccard(representation['representation'])\n",
    "        compression = representation['compression_ratio']\n",
    "        results.append({\n",
    "            'accuracy': accuracy,\n",
    "            'compression': compression,\n",
    "            'n_basis_used': len(representation['basis_indices'])\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hllset-swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
