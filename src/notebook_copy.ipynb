{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HLLSet kernel from: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import hashlib\n",
    "from typing import List, Dict, Tuple, Set, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "# from hllset_swarm.hllset_wrapper import HllSet, HllHashInfo\n",
    "from hllset_swarm.persistence import PersistentLookupTable, PersistentAdjacencyMatrix, CorpusStateManager\n",
    "from hllset_swarm.ingest import LookupTable, ingest_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup Table Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case n-grams are actually n-tokens. In Chinese character is token, so <START> and <END> also characters in chinese language sense\n",
    "\n",
    "So in the context of our system:\n",
    "\n",
    "- **Token** = atomic unit (in Chinese: a character; in English: could be a word/subword)\n",
    "- **n-gram** = sequence of n tokens\n",
    "- **START** and **END** = special single-character tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hllset_swarm.hllset_wrapper import HllSet, HllHashInfo\n",
    "\n",
    "def test_ingest(CORPUS: List[str]) -> Tuple[PersistentAdjacencyMatrix, Dict[str, int], LookupTable, HllSet]:\n",
    "    \"\"\"Test the ingest function with sample corpus\"\"\"\n",
    "    # from hllset_wrapper import HllSet\n",
    "    \n",
    "    print(\"=== Test Ingest Function ===\\n\")\n",
    "    \n",
    "    # Initialize HllSet\n",
    "    hll = HllSet(P=10)\n",
    "    \n",
    "    print(\"Step 1: Ingest corpus\")\n",
    "    adj, tok_id, lut = ingest_corpus(CORPUS, hll)\n",
    "    \n",
    "    print(f\"  Total token sequences in AM: {len(tok_id)}\")\n",
    "    print(f\"  Total edges: {adj._nnz()}\")\n",
    "    print(f\"  HLL cardinality: {hll.count():.0f}\")\n",
    "    \n",
    "    print(\"\\nStep 2: Verify token sequence registration\")\n",
    "    # Show some token sequences and their HLL pairs\n",
    "    print(\"  Sample token sequences:\")\n",
    "    for seq in list(tok_id.keys())[:15]:\n",
    "        pair = lut.get_hll_pair(seq)\n",
    "        print(f\"    '{seq}' (len={len(seq)}) -> Node ID: {tok_id[seq]}, HLL pair: {pair}\")\n",
    "    \n",
    "    print(\"\\nStep 3: Verify extended text processing\")\n",
    "    # Check that START and END are registered\n",
    "    START, END = \"⊢\", \"⊣\"\n",
    "    assert START in tok_id, \"START token not found\"\n",
    "    assert END in tok_id, \"END token not found\"\n",
    "    print(f\"  ✓ START ('{START}') and END ('{END}') tokens registered\")\n",
    "    \n",
    "    # Verify first text decomposition\n",
    "    text1 = CORPUS[0]\n",
    "    tokens = [START] + list(text1) + [END]\n",
    "    print(f\"\\n  Original text: '{text1}'\")\n",
    "    print(f\"  Token sequence: {tokens}\")\n",
    "    print(f\"  Total tokens: {len(tokens)}\")\n",
    "    \n",
    "    # Check 3-token decomposition for first window\n",
    "    tok_a, tok_b, tok_c = tokens[0], tokens[1], tokens[2]\n",
    "    unigram = tok_a\n",
    "    bigram = tok_a + tok_b\n",
    "    trigram = tok_a + tok_b + tok_c\n",
    "    \n",
    "    print(f\"\\n  First 3-token window: [{tok_a}, {tok_b}, {tok_c}]\")\n",
    "    print(f\"    Unigram (1-token): '{unigram}' (Node ID: {tok_id.get(unigram)})\")\n",
    "    print(f\"    Bigram (2-token): '{bigram}' (Node ID: {tok_id.get(bigram)})\")\n",
    "    print(f\"    Trigram (3-token): '{trigram}' (Node ID: {tok_id.get(trigram)})\")\n",
    "    \n",
    "    print(\"\\nStep 4: Verify adjacency matrix structure\")\n",
    "    indices = adj.coalesce().indices()\n",
    "    values = adj.coalesce().values()\n",
    "    \n",
    "    id_to_tok = {v: k for k, v in tok_id.items()}\n",
    "    \n",
    "    print(\"  Sample edges (source -> target: weight):\")\n",
    "    for i in range(min(15, indices.shape[1])):\n",
    "        u, v = indices[0, i].item(), indices[1, i].item()\n",
    "        w = values[i].item()\n",
    "        src_seq = id_to_tok[u]\n",
    "        dst_seq = id_to_tok[v]\n",
    "        print(f\"    '{src_seq}' (len={len(src_seq)}) -> '{dst_seq}' (len={len(dst_seq)}): {w}\")\n",
    "    \n",
    "    print(\"\\nStep 5: Verify LUT hash collisions\")\n",
    "    collisions = lut.get_collisions()\n",
    "    \n",
    "    if len(collisions) == 0:\n",
    "        print(\"  No hash collisions detected\")\n",
    "    else:\n",
    "        print(f\"  Found {len(collisions)} collision(s):\")\n",
    "        for pair, token_set in collisions.items():\n",
    "            print(f\"    {pair}: {token_set}\")\n",
    "    \n",
    "    print(\"\\n=== Test Complete ===\")\n",
    "    \n",
    "    return adj, tok_id, lut, hll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Ingest Function ===\n",
      "\n",
      "Step 1: Ingest corpus\n",
      "  Total token sequences in AM: 24\n",
      "  Total edges: 16\n",
      "  HLL cardinality: 25\n",
      "\n",
      "Step 2: Verify token sequence registration\n",
      "  Sample token sequences:\n",
      "    '⊢' (len=1) -> Node ID: 0, HLL pair: (736, 3)\n",
      "    '⊣' (len=1) -> Node ID: 1, HLL pair: (415, 1)\n",
      "    '⊢人' (len=2) -> Node ID: 2, HLL pair: (290, 1)\n",
      "    '⊢人工' (len=3) -> Node ID: 3, HLL pair: (740, 3)\n",
      "    '人' (len=1) -> Node ID: 4, HLL pair: (235, 1)\n",
      "    '人工' (len=2) -> Node ID: 5, HLL pair: (285, 2)\n",
      "    '人工智' (len=3) -> Node ID: 6, HLL pair: (60, 1)\n",
      "    '工' (len=1) -> Node ID: 7, HLL pair: (208, 1)\n",
      "    '工智' (len=2) -> Node ID: 8, HLL pair: (346, 3)\n",
      "    '工智能' (len=3) -> Node ID: 9, HLL pair: (511, 1)\n",
      "    '智' (len=1) -> Node ID: 10, HLL pair: (291, 3)\n",
      "    '智能' (len=2) -> Node ID: 11, HLL pair: (71, 2)\n",
      "    '智能⊣' (len=3) -> Node ID: 12, HLL pair: (386, 1)\n",
      "    '⊢机' (len=2) -> Node ID: 13, HLL pair: (447, 2)\n",
      "    '⊢机器' (len=3) -> Node ID: 14, HLL pair: (349, 4)\n",
      "\n",
      "Step 3: Verify extended text processing\n",
      "  ✓ START ('⊢') and END ('⊣') tokens registered\n",
      "\n",
      "  Original text: '人工智能'\n",
      "  Token sequence: ['⊢', '人', '工', '智', '能', '⊣']\n",
      "  Total tokens: 6\n",
      "\n",
      "  First 3-token window: [⊢, 人, 工]\n",
      "    Unigram (1-token): '⊢' (Node ID: 0)\n",
      "    Bigram (2-token): '⊢人' (Node ID: 2)\n",
      "    Trigram (3-token): '⊢人工' (Node ID: 3)\n",
      "\n",
      "Step 4: Verify adjacency matrix structure\n",
      "  Sample edges (source -> target: weight):\n",
      "    '⊢' (len=1) -> '⊢人' (len=2): 1.0\n",
      "    '⊢' (len=1) -> '⊢机' (len=2): 1.0\n",
      "    '⊢人' (len=2) -> '⊢人工' (len=3): 1.0\n",
      "    '人' (len=1) -> '人工' (len=2): 1.0\n",
      "    '人工' (len=2) -> '人工智' (len=3): 1.0\n",
      "    '工' (len=1) -> '工智' (len=2): 1.0\n",
      "    '工智' (len=2) -> '工智能' (len=3): 1.0\n",
      "    '智' (len=1) -> '智能' (len=2): 1.0\n",
      "    '智能' (len=2) -> '智能⊣' (len=3): 1.0\n",
      "    '⊢机' (len=2) -> '⊢机器' (len=3): 1.0\n",
      "    '机' (len=1) -> '机器' (len=2): 1.0\n",
      "    '机器' (len=2) -> '机器学' (len=3): 1.0\n",
      "    '器' (len=1) -> '器学' (len=2): 1.0\n",
      "    '器学' (len=2) -> '器学习' (len=3): 1.0\n",
      "    '学' (len=1) -> '学习' (len=2): 1.0\n",
      "\n",
      "Step 5: Verify LUT hash collisions\n",
      "  No hash collisions detected\n",
      "\n",
      "=== Test Complete ===\n",
      "tensor(indices=tensor([[ 0,  0,  2,  4,  5,  7,  8, 10, 11, 13, 15, 16, 18, 19,\n",
      "                        21, 22],\n",
      "                       [ 2, 13,  3,  5,  6,  8,  9, 11, 12, 14, 16, 17, 19, 20,\n",
      "                        22, 23]]),\n",
      "       values=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "                      1., 1.]),\n",
      "       size=(24, 24), nnz=16, layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "# Sample corpus (Chinese text - each character is a token)\n",
    "CORPUS = [\n",
    "    \"人工智能\",\n",
    "    \"机器学习\"\n",
    "]    \n",
    "\n",
    "adj, tok_id, lut, hll = test_ingest(CORPUS)\n",
    "\n",
    "print(adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Persistence System with Incremental Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import duckdb\n",
    "import pickle\n",
    "import zstandard as zstd\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List, Optional, Set\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Usage example for SGS.ai real-time processing\n",
    "def demo_realtime_workflow():\n",
    "    \"\"\"Simulate SGS.ai real-time workflow\"\"\"\n",
    "    # from hllset_wrapper import HllSet\n",
    "    \n",
    "    # Initialize state manager\n",
    "    state_mgr = CorpusStateManager(\"./sgs_corpus_state\")\n",
    "    \n",
    "    # Initialize HLL for this session\n",
    "    hll = HllSet(P=10)\n",
    "    \n",
    "    # === Scenario 1: New data arrives ===\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SCENARIO 1: Ingest new sensory data\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    new_corpus_1 = [\"人工智能\", \"机器学习\"]\n",
    "    state_mgr.ingest_and_merge(new_corpus_1, hll)\n",
    "    \n",
    "    # === Scenario 2: More data arrives (incremental) ===\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SCENARIO 2: Ingest more data (incremental merge)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    new_corpus_2 = [\"深度学习\", \"神经网络\"]\n",
    "    state_mgr.ingest_and_merge(new_corpus_2, hll)\n",
    "    \n",
    "    # === Scenario 3: Restore from HLLSet ===\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SCENARIO 3: Restore text from HLLSet\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create HLLSet from some text (simulating self-generation loop output)\n",
    "    restoration_hll = HllSet(P=10)\n",
    "    test_text = \"人工智能\"\n",
    "    for char in test_text:\n",
    "        restoration_hll.add(char)\n",
    "    \n",
    "    # Retrieve data for restoration\n",
    "    pruned_adj, pruned_tok_id, tokens_by_pair = state_mgr.retrieve_for_restoration(restoration_hll)\n",
    "    \n",
    "    print(f\"\\nReady for order reconstruction:\")\n",
    "    print(f\"  Pruned AM: {pruned_adj.shape}\")\n",
    "    print(f\"  Token candidates: {sum(len(v) for v in tokens_by_pair.values())}\")\n",
    "    \n",
    "    # Close connections\n",
    "    state_mgr.close()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Demo complete!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     demo_realtime_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SCENARIO 1: Ingest new sensory data\n",
      "============================================================\n",
      "\n",
      "=== Ingesting new corpus ===\n",
      "Memory: 24 tokens, 16 edges\n",
      "\n",
      "=== Merging with persistent storage ===\n",
      "Merged 24 token entries into persistent LUT\n",
      "Loaded AM: torch.Size([43, 43]), 30 edges\n",
      "Merging adjacency matrices...\n",
      "Saved AM: torch.Size([43, 43]), 30 edges, compressed to 0.7 KB\n",
      "Merge complete: 43 tokens, 30 edges\n",
      "Merge complete!\n",
      "\n",
      "\n",
      "============================================================\n",
      "SCENARIO 2: Ingest more data (incremental merge)\n",
      "============================================================\n",
      "\n",
      "=== Ingesting new corpus ===\n",
      "Memory: 24 tokens, 16 edges\n",
      "\n",
      "=== Merging with persistent storage ===\n",
      "Merged 24 token entries into persistent LUT\n",
      "Loaded AM: torch.Size([43, 43]), 30 edges\n",
      "Merging adjacency matrices...\n",
      "Saved AM: torch.Size([43, 43]), 30 edges, compressed to 0.9 KB\n",
      "Merge complete: 43 tokens, 30 edges\n",
      "Merge complete!\n",
      "\n",
      "\n",
      "============================================================\n",
      "SCENARIO 3: Restore text from HLLSet\n",
      "============================================================\n",
      "\n",
      "=== Retrieving data for restoration ===\n",
      "HLLSet contains 4 unique (reg, run) pairs\n",
      "Retrieved token candidates for 0 pairs\n",
      "Loaded AM: torch.Size([43, 43]), 30 edges\n",
      "Disambiguated to 2 tokens\n",
      "Pruned AM: 0 edges, 2 nodes\n",
      "\n",
      "Ready for order reconstruction:\n",
      "  Pruned AM: torch.Size([2, 2])\n",
      "  Token candidates: 0\n",
      "\n",
      "============================================================\n",
      "Demo complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "demo_realtime_workflow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hllset-swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
