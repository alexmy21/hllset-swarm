{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hllset_swarm.trajectory import SwarmProgram\n",
    "from hllset_swarm.io.env import Environment\n",
    "\n",
    "__all__ = [\"SwarmProgram\", \"Environment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hllset_swarm.hllset_wrapper import HllSet, HllHashInfo\n",
    "\n",
    "hll = HllSet(P=6)\n",
    "\n",
    "# Add single element\n",
    "hash_info = hll.add(\"人\")\n",
    "print(f\"Token: {hash_info.token}, Hash: {hash_info.hash_value}, Bin: {hash_info.bin}, Idx: {hash_info.idx}\")\n",
    "print(f\"HLL Pair: {hash_info.hll_pair}\")  # (bin, idx)\n",
    "\n",
    "# Add batch\n",
    "results = hll.add_batch([\"工\", \"智\", \"能\"])\n",
    "for info in results:\n",
    "    print(f\"Token hash info: {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import hashlib\n",
    "from typing import List, Dict, Tuple, Set, Optional\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup Table Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LookupTable:\n",
    "    \"\"\"\n",
    "    Hash-based lookup table for tokens with collision handling.\n",
    "    \n",
    "    Structure:\n",
    "        hll_pair (reg, run) -> {\n",
    "            'tokens': set of tokens with this (reg, run),\n",
    "            'hashes': set of original hash values\n",
    "        }\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.table: Dict[Tuple[int, int], Dict] = {}\n",
    "        self.token_to_pair: Dict[str, Tuple[int, int]] = {}\n",
    "    \n",
    "    def add_token(self, token: str, hash_info: HllHashInfo) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Add token to LUT using HllHashInfo.\n",
    "        \n",
    "        Args:\n",
    "            token: Token string\n",
    "            hash_info: HllHashInfo from HllSet.add()\n",
    "        \n",
    "        Returns:\n",
    "            (reg, run) pair\n",
    "        \"\"\"\n",
    "        pair = hash_info.hll_pair  # (bin, idx)\n",
    "        \n",
    "        if pair not in self.table:\n",
    "            self.table[pair] = {\n",
    "                'tokens': set(),  # Changed to set\n",
    "                'hashes': set()   # Changed to set\n",
    "            }\n",
    "        \n",
    "        # Set.add() is idempotent, no need to check membership\n",
    "        self.table[pair]['tokens'].add(token)\n",
    "        self.table[pair]['hashes'].add(hash_info.hash_value)\n",
    "        self.token_to_pair[token] = pair\n",
    "        \n",
    "        return pair\n",
    "    \n",
    "    def get_tokens_by_hll(self, reg: int, run: int) -> List[str]:\n",
    "        \"\"\"Get all tokens that map to this (reg, run) pair\"\"\"\n",
    "        pair = (reg, run)\n",
    "        if pair in self.table:\n",
    "            return list(self.table[pair]['tokens'])  # Convert set to list for return\n",
    "        return []\n",
    "    \n",
    "    def get_hll_pair(self, token: str) -> Optional[Tuple[int, int]]:\n",
    "        \"\"\"Get HLLSet representation for token\"\"\"\n",
    "        return self.token_to_pair.get(token)\n",
    "    \n",
    "    def get_collision_count(self) -> int:\n",
    "        \"\"\"Return number of (reg, run) pairs with multiple tokens\"\"\"\n",
    "        return sum(1 for data in self.table.values() if len(data['tokens']) > 1)\n",
    "    \n",
    "    def get_collisions(self) -> Dict[Tuple[int, int], set]:\n",
    "        \"\"\"Return all (reg, run) pairs that have collisions\"\"\"\n",
    "        return {\n",
    "            pair: data['tokens'] \n",
    "            for pair, data in self.table.items() \n",
    "            if len(data['tokens']) > 1\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_corpus(corpus: List[str], hll: HllSet) -> Tuple[torch.Tensor, Dict[str, int], LookupTable]:\n",
    "    \"\"\"\n",
    "    Ingest corpus with proper START/END handling and sliding 3-token window.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. For each text: wrap with START+text+END (START and END are single tokens)\n",
    "    2. Slide 3-token window over extended text with step=1\n",
    "    3. Each 3-token window (a,b,c) decomposes into: {(a), (a,b), (a,b,c)}\n",
    "       - 1-token (unigram): a\n",
    "       - 2-token (bigram): ab\n",
    "       - 3-token (trigram): abc\n",
    "    4. Build adjacency matrix and lookup table\n",
    "    \n",
    "    Args:\n",
    "        corpus: List of text strings (each text is a sequence of tokens/characters)\n",
    "        hll: HllSet instance for encoding tokens\n",
    "    \n",
    "    Returns:\n",
    "        adj: Sparse adjacency matrix with frequency edges\n",
    "        tok_id: token_sequence → AM node ID mapping\n",
    "        lut: LookupTable with token_sequence → HLL mappings\n",
    "    \"\"\"\n",
    "    START = \"⊢\"  # Unicode start symbol (single token)\n",
    "    END = \"⊣\"    # Unicode end symbol (single token)\n",
    "    \n",
    "    lut = LookupTable()\n",
    "    tok_id = {}\n",
    "    edge_freq = defaultdict(int)\n",
    "    \n",
    "    # Register special tokens (they are single tokens like any other character)\n",
    "    start_info = hll.add(START)\n",
    "    end_info = hll.add(END)\n",
    "    lut.add_token(START, start_info)\n",
    "    lut.add_token(END, end_info)\n",
    "    tok_id[START] = 0\n",
    "    tok_id[END] = 1\n",
    "    \n",
    "    for text in corpus:\n",
    "        text = text.strip()\n",
    "        if len(text) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Create token sequence: [START, tok1, tok2, ..., tokN, END]\n",
    "        tokens = [START] + list(text) + [END]\n",
    "        \n",
    "        # Slide 3-token window over token sequence (step=1)\n",
    "        for i in range(len(tokens) - 2):\n",
    "            # Extract 3-token window\n",
    "            tok_a = tokens[i]      # Token at position i\n",
    "            tok_b = tokens[i + 1]  # Token at position i+1\n",
    "            tok_c = tokens[i + 2]  # Token at position i+2\n",
    "            \n",
    "            # Decompose into n-token sequences (n-grams)\n",
    "            unigram = tok_a              # 1-token sequence\n",
    "            bigram = tok_a + tok_b       # 2-token sequence\n",
    "            trigram = tok_a + tok_b + tok_c  # 3-token sequence\n",
    "            \n",
    "            # Register all n-token sequences in HLL, LUT, and AM\n",
    "            for ngram in [unigram, bigram, trigram]:\n",
    "                if ngram not in tok_id:\n",
    "                    # Add to HLL and get hash info\n",
    "                    hash_info = hll.add(ngram)\n",
    "                    # Register in LUT\n",
    "                    lut.add_token(ngram, hash_info)\n",
    "                    # Assign AM node ID\n",
    "                    tok_id[ngram] = len(tok_id)\n",
    "            \n",
    "            # Build edges: unigram → bigram → trigram\n",
    "            id_1 = tok_id[unigram]\n",
    "            id_2 = tok_id[bigram]\n",
    "            id_3 = tok_id[trigram]\n",
    "            \n",
    "            edge_freq[(id_1, id_2)] += 1\n",
    "            edge_freq[(id_2, id_3)] += 1\n",
    "    \n",
    "    # Build sparse adjacency matrix\n",
    "    N = len(tok_id)\n",
    "    adj_u, adj_v, adj_w = [], [], []\n",
    "    \n",
    "    for (u, v), freq in edge_freq.items():\n",
    "        adj_u.append(u)\n",
    "        adj_v.append(v)\n",
    "        adj_w.append(float(freq))\n",
    "    \n",
    "    adj = torch.sparse_coo_tensor(\n",
    "        indices=torch.tensor([adj_u, adj_v], dtype=torch.long),\n",
    "        values=torch.tensor(adj_w, dtype=torch.float32),\n",
    "        size=(N, N)\n",
    "    ).coalesce()\n",
    "    \n",
    "    return adj, tok_id, lut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case n-grams are actually n-tokens. In Chinese character is token, so <START> and <END> also characters in chinese language sense\n",
    "\n",
    "So in the context of our system:\n",
    "\n",
    "- **Token** = atomic unit (in Chinese: a character; in English: could be a word/subword)\n",
    "- **n-gram** = sequence of n tokens\n",
    "- **START** and **END** = special single-character tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_ingest():\n",
    "    \"\"\"Test the ingest function with sample corpus\"\"\"\n",
    "    # from hllset_wrapper import HllSet\n",
    "    \n",
    "    print(\"=== Test Ingest Function ===\\n\")\n",
    "    \n",
    "    # Sample corpus (Chinese text - each character is a token)\n",
    "    CORPUS = [\n",
    "        \"人工智能\",\n",
    "        \"机器学习\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize HllSet\n",
    "    hll = HllSet(P=10)\n",
    "    \n",
    "    print(\"Step 1: Ingest corpus\")\n",
    "    adj, tok_id, lut = ingest_corpus(CORPUS, hll)\n",
    "    \n",
    "    print(f\"  Total token sequences in AM: {len(tok_id)}\")\n",
    "    print(f\"  Total edges: {adj._nnz()}\")\n",
    "    print(f\"  HLL cardinality: {hll.count():.0f}\")\n",
    "    \n",
    "    print(\"\\nStep 2: Verify token sequence registration\")\n",
    "    # Show some token sequences and their HLL pairs\n",
    "    print(\"  Sample token sequences:\")\n",
    "    for seq in list(tok_id.keys())[:15]:\n",
    "        pair = lut.get_hll_pair(seq)\n",
    "        print(f\"    '{seq}' (len={len(seq)}) -> Node ID: {tok_id[seq]}, HLL pair: {pair}\")\n",
    "    \n",
    "    print(\"\\nStep 3: Verify extended text processing\")\n",
    "    # Check that START and END are registered\n",
    "    START, END = \"⊢\", \"⊣\"\n",
    "    assert START in tok_id, \"START token not found\"\n",
    "    assert END in tok_id, \"END token not found\"\n",
    "    print(f\"  ✓ START ('{START}') and END ('{END}') tokens registered\")\n",
    "    \n",
    "    # Verify first text decomposition\n",
    "    text1 = CORPUS[0]\n",
    "    tokens = [START] + list(text1) + [END]\n",
    "    print(f\"\\n  Original text: '{text1}'\")\n",
    "    print(f\"  Token sequence: {tokens}\")\n",
    "    print(f\"  Total tokens: {len(tokens)}\")\n",
    "    \n",
    "    # Check 3-token decomposition for first window\n",
    "    tok_a, tok_b, tok_c = tokens[0], tokens[1], tokens[2]\n",
    "    unigram = tok_a\n",
    "    bigram = tok_a + tok_b\n",
    "    trigram = tok_a + tok_b + tok_c\n",
    "    \n",
    "    print(f\"\\n  First 3-token window: [{tok_a}, {tok_b}, {tok_c}]\")\n",
    "    print(f\"    Unigram (1-token): '{unigram}' (Node ID: {tok_id.get(unigram)})\")\n",
    "    print(f\"    Bigram (2-token): '{bigram}' (Node ID: {tok_id.get(bigram)})\")\n",
    "    print(f\"    Trigram (3-token): '{trigram}' (Node ID: {tok_id.get(trigram)})\")\n",
    "    \n",
    "    print(\"\\nStep 4: Verify adjacency matrix structure\")\n",
    "    indices = adj.coalesce().indices()\n",
    "    values = adj.coalesce().values()\n",
    "    \n",
    "    id_to_tok = {v: k for k, v in tok_id.items()}\n",
    "    \n",
    "    print(\"  Sample edges (source -> target: weight):\")\n",
    "    for i in range(min(15, indices.shape[1])):\n",
    "        u, v = indices[0, i].item(), indices[1, i].item()\n",
    "        w = values[i].item()\n",
    "        src_seq = id_to_tok[u]\n",
    "        dst_seq = id_to_tok[v]\n",
    "        print(f\"    '{src_seq}' (len={len(src_seq)}) -> '{dst_seq}' (len={len(dst_seq)}): {w}\")\n",
    "    \n",
    "    print(\"\\nStep 5: Verify LUT hash collisions\")\n",
    "    collisions = lut.get_collisions()\n",
    "    \n",
    "    if len(collisions) == 0:\n",
    "        print(\"  No hash collisions detected\")\n",
    "    else:\n",
    "        print(f\"  Found {len(collisions)} collision(s):\")\n",
    "        for pair, token_set in collisions.items():\n",
    "            print(f\"    {pair}: {token_set}\")\n",
    "    \n",
    "    print(\"\\n=== Test Complete ===\")\n",
    "    \n",
    "    return adj, tok_id, lut, hll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Ingest Function ===\n",
      "\n",
      "Step 1: Ingest corpus\n",
      "  Total token sequences in AM: 24\n",
      "  Total edges: 16\n",
      "  HLL cardinality: 25\n",
      "\n",
      "Step 2: Verify token sequence registration\n",
      "  Sample token sequences:\n",
      "    '⊢' (len=1) -> Node ID: 0, HLL pair: (736, 3)\n",
      "    '⊣' (len=1) -> Node ID: 1, HLL pair: (415, 1)\n",
      "    '⊢人' (len=2) -> Node ID: 2, HLL pair: (290, 1)\n",
      "    '⊢人工' (len=3) -> Node ID: 3, HLL pair: (740, 3)\n",
      "    '人' (len=1) -> Node ID: 4, HLL pair: (235, 1)\n",
      "    '人工' (len=2) -> Node ID: 5, HLL pair: (285, 2)\n",
      "    '人工智' (len=3) -> Node ID: 6, HLL pair: (60, 1)\n",
      "    '工' (len=1) -> Node ID: 7, HLL pair: (208, 1)\n",
      "    '工智' (len=2) -> Node ID: 8, HLL pair: (346, 3)\n",
      "    '工智能' (len=3) -> Node ID: 9, HLL pair: (511, 1)\n",
      "    '智' (len=1) -> Node ID: 10, HLL pair: (291, 3)\n",
      "    '智能' (len=2) -> Node ID: 11, HLL pair: (71, 2)\n",
      "    '智能⊣' (len=3) -> Node ID: 12, HLL pair: (386, 1)\n",
      "    '⊢机' (len=2) -> Node ID: 13, HLL pair: (447, 2)\n",
      "    '⊢机器' (len=3) -> Node ID: 14, HLL pair: (349, 4)\n",
      "\n",
      "Step 3: Verify extended text processing\n",
      "  ✓ START ('⊢') and END ('⊣') tokens registered\n",
      "\n",
      "  Original text: '人工智能'\n",
      "  Token sequence: ['⊢', '人', '工', '智', '能', '⊣']\n",
      "  Total tokens: 6\n",
      "\n",
      "  First 3-token window: [⊢, 人, 工]\n",
      "    Unigram (1-token): '⊢' (Node ID: 0)\n",
      "    Bigram (2-token): '⊢人' (Node ID: 2)\n",
      "    Trigram (3-token): '⊢人工' (Node ID: 3)\n",
      "\n",
      "Step 4: Verify adjacency matrix structure\n",
      "  Sample edges (source -> target: weight):\n",
      "    '⊢' (len=1) -> '⊢人' (len=2): 1.0\n",
      "    '⊢' (len=1) -> '⊢机' (len=2): 1.0\n",
      "    '⊢人' (len=2) -> '⊢人工' (len=3): 1.0\n",
      "    '人' (len=1) -> '人工' (len=2): 1.0\n",
      "    '人工' (len=2) -> '人工智' (len=3): 1.0\n",
      "    '工' (len=1) -> '工智' (len=2): 1.0\n",
      "    '工智' (len=2) -> '工智能' (len=3): 1.0\n",
      "    '智' (len=1) -> '智能' (len=2): 1.0\n",
      "    '智能' (len=2) -> '智能⊣' (len=3): 1.0\n",
      "    '⊢机' (len=2) -> '⊢机器' (len=3): 1.0\n",
      "    '机' (len=1) -> '机器' (len=2): 1.0\n",
      "    '机器' (len=2) -> '机器学' (len=3): 1.0\n",
      "    '器' (len=1) -> '器学' (len=2): 1.0\n",
      "    '器学' (len=2) -> '器学习' (len=3): 1.0\n",
      "    '学' (len=1) -> '学习' (len=2): 1.0\n",
      "\n",
      "Step 5: Verify LUT hash collisions\n",
      "  No hash collisions detected\n",
      "\n",
      "=== Test Complete ===\n",
      "tensor(indices=tensor([[ 0,  0,  2,  4,  5,  7,  8, 10, 11, 13, 15, 16, 18, 19,\n",
      "                        21, 22],\n",
      "                       [ 2, 13,  3,  5,  6,  8,  9, 11, 12, 14, 16, 17, 19, 20,\n",
      "                        22, 23]]),\n",
      "       values=tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "                      1., 1.]),\n",
      "       size=(24, 24), nnz=16, layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "adj, tok_id, lut, hll = test_ingest()\n",
    "\n",
    "print(adj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hllset-swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
