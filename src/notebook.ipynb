{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HLLSet kernel from: None\n"
     ]
    }
   ],
   "source": [
    "from hllset_swarm.trajectory import SwarmProgram\n",
    "from hllset_swarm.io.env import Environment\n",
    "\n",
    "__all__ = [\"SwarmProgram\", \"Environment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 人, Hash: 2111899233249189501, Bin: 8, Idx: 1\n",
      "HLL Pair: (8, 1)\n",
      "Token hash info: HllHashInfo(token='工', hash_value=1872328049933700185, bin=7, idx=1)\n",
      "Token hash info: HllHashInfo(token='智', hash_value=2615720522607693844, bin=10, idx=3)\n",
      "Token hash info: HllHashInfo(token='能', hash_value=216904342312054849, bin=1, idx=1)\n"
     ]
    }
   ],
   "source": [
    "from hllset_swarm.hllset_wrapper import HllSet\n",
    "\n",
    "hll = HllSet(P=5)\n",
    "\n",
    "# Add single element\n",
    "hash_info = hll.add(\"人\")\n",
    "print(f\"Token: {hash_info.token}, Hash: {hash_info.hash_value}, Bin: {hash_info.bin}, Idx: {hash_info.idx}\")\n",
    "print(f\"HLL Pair: {hash_info.hll_pair}\")  # (bin, idx)\n",
    "\n",
    "# Add batch\n",
    "results = hll.add_batch([\"工\", \"智\", \"能\"])\n",
    "for info in results:\n",
    "    print(f\"Token hash info: {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo\n",
    "\n",
    "Below is a single-file, zero-dependency Python script that:\n",
    "\n",
    "- builds 3 dummy inverted indices (1-, 2-, 3-gram)\n",
    "- creates a given HLLSet bit-vector\n",
    "- trains a tiny TRM actuator (shared Wτ, Wρ) on fake “bad→good” pairs\n",
    "- tests it until reconstruction error > threshold → triggers on-line tuning\n",
    "- hot-swaps the lattice and resumes with lower error\n",
    "\n",
    "Run it anywhere: python dummy_self_tune.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Dummy self-tuning TRM actuator\n",
    "- 3 inverted indices (1-,2-,3-gram)\n",
    "- given HLLSet bit-vector\n",
    "- train → test → trigger tune → hot-swap\n",
    "No external deps – pure Python + built-in random\n",
    "\"\"\"\n",
    "import random, math, time, json, gzip, os\n",
    "from typing import Dict, List, Set, Tuple\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 1.  CONFIG ----------\n",
    "VOCAB_1G      = 5000          # dummy 1-grams\n",
    "VOCAB_2G      = 3000          # dummy 2-grams\n",
    "VOCAB_3G      = 2000          # dummy 3-grams\n",
    "REGISTERS     = 1024          # P=10\n",
    "HASH_WIDTH  = 32            # 32-bit hash\n",
    "BEAM          = 5\n",
    "TOLERANCE   = 0.05          # 5 % BER\n",
    "TUNE_SAMPLES= 20            # mini-batch for tuning\n",
    "MAX_PASS    = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 2.  DUMMY INVERTED INDICES ----------\n",
    "def hash32(x: str) -> int:\n",
    "    return abs(hash(x)) & ((1 << HASH_WIDTH) - 1)\n",
    "\n",
    "def build_index(vocab: List[str], name: str) -> Dict[str, dict]:\n",
    "    idx = {}\n",
    "    for tok in vocab:\n",
    "        h = hash32(tok)\n",
    "        reg = h % REGISTERS\n",
    "        run = (h >> 10) & 31   # 0-31\n",
    "        idx[tok] = {\"hash\": h, \"reg\": reg, \"run\": run}\n",
    "    return idx\n",
    "\n",
    "print(\"Building dummy indices…\")\n",
    "idx1 = build_index([f\"1g{i:05d}\" for i in range(VOCAB_1G)], \"1g\")\n",
    "idx2 = build_index([f\"2g{i:05d}\" for i in range(VOCAB_2G)], \"2g\")\n",
    "idx3 = build_index([f\"3g{i:05d}\" for i in range(VOCAB_3G)], \"3g\")\n",
    "print(f\"Indices ready: 1g={len(idx1)}, 2g={len(idx2)}, 3g={len(idx3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 3.  GIVEN HLLSet BIT-VECTOR ----------\n",
    "def given_hllset() -> List[int]:\n",
    "    \"\"\"returns bit-vector 0/1 length REGISTERS\"\"\"\n",
    "    # fake: 60 % density, mutually exclusive hashes\n",
    "    bits = [0] * REGISTERS\n",
    "    for tok in list(idx1.values())[:3000] + list(idx2.values())[:2000] + list(idx3.values())[:1000]:\n",
    "        bits[tok[\"reg\"]] |= 1 << tok[\"run\"]\n",
    "    return bits\n",
    "\n",
    "ORIGINAL_BITS = given_hllset()\n",
    "print(f\"Given HLLSet: {sum(ORIGINAL_BITS)} bits set / {REGISTERS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 4.  TOKEN ↔ BIT-VECTOR helpers ----------\n",
    "def bits_to_tokens(bits: List[int], idx: Dict[str, dict]) -> List[str]:\n",
    "    \"\"\"naïve cover: return tokens whose (reg,run) is set\"\"\"\n",
    "    out = []\n",
    "    for tok, info in idx.items():\n",
    "        if bits[info[\"reg\"]] & (1 << info[\"run\"]):\n",
    "            out.append(tok)\n",
    "    return out\n",
    "\n",
    "def tokens_to_bits(tokens: List[str], idx: Dict[str, dict]) -> List[int]:\n",
    "    bits = [0] * REGISTERS\n",
    "    for tok in tokens:\n",
    "        info = idx[tok]\n",
    "        bits[info[\"reg\"]] |= 1 << info[\"run\"]\n",
    "    return bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 5.  RECONSTRUCTION LOSS (Jaccard) ----------\n",
    "def reconstruction_loss(pred_tokens: List[str], orig_bits: List[int]) -> float:\n",
    "    pred_bits = tokens_to_bits(pred_tokens, {**idx1, **idx2, **idx3})\n",
    "    intersect = sum(a & b for a, b in zip(orig_bits, pred_bits))\n",
    "    union = sum(a | b for a, b in zip(orig_bits, pred_bits))\n",
    "    return 1.0 - (intersect / max(union, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 6.  TINY TRM ACTUATOR ----------\n",
    "class TRMActuator:\n",
    "    def __init__(self, n: int):\n",
    "        self.n = n\n",
    "        self.Wτ = self._random_sparse()\n",
    "        self.Wρ = self.Wτ * 0.3\n",
    "        self.alpha, self.beta, self.gamma = 0.2, 0.15, 0.05\n",
    "        self.eta = 0.02\n",
    "\n",
    "    def _random_sparse(self, density=0.001):\n",
    "        # import torch\n",
    "        idx = torch.randint(0, n, (2, int(n * n * density)))\n",
    "        val = torch.rand(idx.shape[1])\n",
    "        return torch.sparse_coo_tensor(idx, val, (n, n)).coalesce()\n",
    "\n",
    "    def step(self, z: torch.Tensor, teacher: torch.Tensor) -> torch.Tensor:\n",
    "        # import torch\n",
    "        cognitive = torch.sparse.mm(self.Wτ, z.unsqueeze(1)).squeeze()\n",
    "        exclusion = torch.sparse.mm(self.Wρ, z.unsqueeze(1)).squeeze()\n",
    "        z_new = z + self.alpha * cognitive - self.beta * exclusion + self.gamma * teacher\n",
    "        return torch.clamp(z_new, 0.0, 1.0)\n",
    "\n",
    "    def hebb_update(self, z: torch.Tensor, loss: float):\n",
    "        # import torch\n",
    "        outer = loss * torch.outer(z, z).flatten()\n",
    "        idx = self.Wτ.coalesce().indices()\n",
    "        delta = torch.sparse_coo_tensor(idx, outer, self.Wτ.shape)\n",
    "        self.Wτ += delta\n",
    "        self.Wρ += delta * 0.3\n",
    "        self.Wτ = self.Wτ.coalesce()\n",
    "        self.Wρ = self.Wρ.coalesce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 7.  BASE TRAINING (bad → good pairs) ----------\n",
    "def base_training(actuator: TRMActuator, epochs: int = 3):\n",
    "    print(\"Base training…\")\n",
    "    for epoch in range(epochs):\n",
    "        # fake: create “bad” cover and “good” cover\n",
    "        bad_bits = [random.randint(0, 1) for _ in range(REGISTERS)]\n",
    "        good_bits = ORIGINAL_BITS\n",
    "        z = torch.tensor(bad_bits, dtype=torch.float32)\n",
    "        teacher = torch.tensor(good_bits, dtype=torch.float32)\n",
    "        for pass_ in range(MAX_PASS):\n",
    "            z = actuator.step(z, teacher)\n",
    "        loss = reconstruction_loss(bits_to_tokens(z, {**idx1, **idx2, **idx3}), good_bits)\n",
    "        actuator.hebb_update(z, loss)\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"  epoch {epoch} loss={loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 8.  TEST LOOP (until threshold breach) ----------\n",
    "def test_until_fail(actuator: TRMActuator, max_tests: int = 100):\n",
    "    print(\"Testing (until fail)...\")\n",
    "    for step in range(max_tests):\n",
    "        # generate noisy input\n",
    "        noisy = [ORIGINAL_BITS[i] if random.random() < 0.95 else 1 - ORIGINAL_BITS[i] for i in range(REGISTERS)]\n",
    "        z = torch.tensor(noisy, dtype=torch.float32)\n",
    "        for pass_ in range(MAX_PASS):\n",
    "            z = actuator.step(z, torch.zeros_like(z))  # no teacher during test\n",
    "        pred_tokens = bits_to_tokens(z, {**idx1, **idx2, **idx3})\n",
    "        loss = reconstruction_loss(pred_tokens, ORIGINAL_BITS)\n",
    "        print(f\"  test {step} loss={loss:.4f}\", end=\"\\r\")\n",
    "        if loss > TOLERANCE:\n",
    "            print(f\"\\n>>> TOLERANCE BREACHED ({loss:.4f} > {TOLERANCE}) – triggering tune-up!\")\n",
    "            return step, loss\n",
    "    print(\"\\n>>> all tests passed – no tune-up needed\")\n",
    "    return -1, 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 9.  ON-LINE TUNING EPISODE ----------\n",
    "def tune_up(actuator: TRMActuator, fail_step: int, fail_loss: float):\n",
    "    print(\"On-line tuning episode…\")\n",
    "    # collect last 50 failing samples\n",
    "    samples = []\n",
    "    for _ in range(TUNE_SAMPLES):\n",
    "        noisy = [ORIGINAL_BITS[i] if random.random() < 0.95 else 1 - ORIGINAL_BITS[i] for i in range(REGISTERS)]\n",
    "        samples.append(noisy)\n",
    "    # train on those 50\n",
    "    for noisy in samples:\n",
    "        z = torch.tensor(noisy, dtype=torch.float32)\n",
    "        teacher = torch.tensor(ORIGINAL_BITS, dtype=torch.float32)\n",
    "        for pass_ in range(MAX_PASS):\n",
    "            z = actuator.step(z, teacher)\n",
    "        loss = reconstruction_loss(bits_to_tokens(z, {**idx1, **idx2, **idx3}), ORIGINAL_BITS)\n",
    "        actuator.hebb_update(z, loss)\n",
    "    print(\">>> tune-up complete – new lattice loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 10.  DEMO RUN ----------\n",
    "# if __name__ == \"__main__\":\n",
    "random.seed(42)\n",
    "actuator = TRMActuator(REGISTERS)\n",
    "base_training(actuator, epochs=3)\n",
    "fail_step, fail_loss = test_until_fail(actuator, max_tests=20)\n",
    "if fail_step >= 0:\n",
    "    tune_up(actuator, fail_step, fail_loss)\n",
    "# final test\n",
    "final_loss = reconstruction_loss(bits_to_tokens(actuator.step(torch.tensor(ORIGINAL_BITS, dtype=torch.float32), torch.zeros_like(torch.tensor(ORIGINAL_BITS, dtype=torch.float32)), {**idx1, **idx2, **idx3}), ORIGINAL_BITS))\n",
    "print(f\"\\nFinal loss after tune-up: {final_loss:.4f} (tolerance {TOLERANCE})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hllset-swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
