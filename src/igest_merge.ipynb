{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest new data and merge it into existing SGS.ai structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Imported binding MainInclude.eval was undeclared at import time during import to Main.\n",
      "WARNING: import of MainInclude.eval into Main conflicts with an existing identifier; ignored.\n",
      "WARNING: Imported binding MainInclude.include was undeclared at import time during import to Main.\n",
      "WARNING: import of MainInclude.include into Main conflicts with an existing identifier; ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HLLSet kernel from: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import hashlib\n",
    "from typing import List, Dict, Tuple, Set, Optional\n",
    "from collections import defaultdict\n",
    "from hllset_swarm.hllset_wrapper import HllSet, HllHashInfo\n",
    "\n",
    "from hllset_swarm.ingest import CorpusState\n",
    "from hllset_swarm.persistence import PersistenceManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Examples\n",
    "### Example 1: Single Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ingesting 2 texts ===\n",
      "Text 1: ⊢人工智能⊣\n",
      "  Text HLL cardinality: 13\n",
      "Text 2: ⊢机器学习⊣\n",
      "  Text HLL cardinality: 14\n",
      "\n",
      "Global state:\n",
      "  Total unique tokens: 24\n",
      "  Total HLLSets: 2\n",
      "  Total edges: 9\n",
      "  Master HLL cardinality: 25\n",
      "\n",
      "Adjacency Matrix:\n",
      "tensor(indices=tensor([[1, 1, 1, 1, 2, 2, 2, 3, 3],\n",
      "                       [1, 2, 3, 4, 1, 3, 4, 1, 2]]),\n",
      "       values=tensor([1., 3., 2., 1., 3., 1., 1., 2., 2.]),\n",
      "       size=(5, 5), nnz=9, layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "corpus1 = [\"人工智能\", \"机器学习\"]\n",
    "corpus_state = CorpusState(P=10)\n",
    "corpus_state.ingest_corpus(corpus1)\n",
    "\n",
    "print(\"\\nAdjacency Matrix:\")\n",
    "adj_matrix, token_to_idx = corpus_state.get_adjacency_matrix()\n",
    "print(adj_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Incremental Merge (The Key Feature!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ingesting 2 texts ===\n",
      "Text 1: ⊢人工智能⊣\n",
      "  Text HLL cardinality: 13\n",
      "Text 2: ⊢机器学习⊣\n",
      "  Text HLL cardinality: 14\n",
      "\n",
      "Global state:\n",
      "  Total unique tokens: 24\n",
      "  Total HLLSets: 4\n",
      "  Total edges: 9\n",
      "  Master HLL cardinality: 25\n",
      "After corpus1: [HllSet(P=10, count=13, tau=0.700, rho=0.210), HllSet(P=10, count=14, tau=0.700, rho=0.210), HllSet(P=10, count=13, tau=0.700, rho=0.210), HllSet(P=10, count=14, tau=0.700, rho=0.210)] hllsets, defaultdict(<class 'int'>, {(3, 1): 4, (1, 3): 4, (1, 2): 6, (2, 1): 6, (3, 2): 4, (2, 4): 2, (2, 3): 2, (1, 4): 2, (1, 1): 2}) edges\n",
      "\n",
      "=== Ingesting 2 texts ===\n",
      "Text 1: ⊢深度学习⊣\n",
      "  Text HLL cardinality: 12\n",
      "Text 2: ⊢人工智能⊣\n",
      "  Text HLL cardinality: 13\n",
      "\n",
      "Global state:\n",
      "  Total unique tokens: 32\n",
      "  Total HLLSets: 6\n",
      "  Total edges: 9\n",
      "  Master HLL cardinality: 32\n",
      "After corpus1: [HllSet(P=10, count=13, tau=0.700, rho=0.210), HllSet(P=10, count=14, tau=0.700, rho=0.210), HllSet(P=10, count=13, tau=0.700, rho=0.210), HllSet(P=10, count=14, tau=0.700, rho=0.210), HllSet(P=10, count=12, tau=0.700, rho=0.210), HllSet(P=10, count=13, tau=0.700, rho=0.210)] hllsets, defaultdict(<class 'int'>, {(3, 1): 8, (1, 3): 6, (1, 2): 9, (2, 1): 8, (3, 2): 5, (2, 4): 2, (2, 3): 3, (1, 4): 2, (1, 1): 5}) edges\n",
      "Frequency of '人': 3\n",
      "Master HLL cardinality: 32\n"
     ]
    }
   ],
   "source": [
    "# First ingestion\n",
    "corpus1 = [\"人工智能\", \"机器学习\"]\n",
    "corpus_state.ingest_corpus(corpus1)\n",
    "\n",
    "print(f\"After corpus1: {corpus_state.hllsets} hllsets, {corpus_state.edge_freq} edges\")\n",
    "\n",
    "# Second ingestion - MERGES with first!\n",
    "corpus2 = [\"深度学习\", \"人工智能\"]  # Note: \"人工智能\" overlaps\n",
    "corpus_state.ingest_corpus(corpus2)\n",
    "\n",
    "print(f\"After corpus1: {corpus_state.hllsets} hllsets, {corpus_state.edge_freq} edges\")\n",
    "\n",
    "# Check that \"人工智能\" has increased frequency\n",
    "print(f\"Frequency of '人': {corpus_state.lut.token_frequency['人']}\")  # Should be 2\n",
    "\n",
    "master_hll = corpus_state.get_master_hll()\n",
    "print(f\"Master HLL cardinality: {master_hll.count():.0f}\")  # Should reflect unique tokens across both corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Retrieve Specific Text's HLLSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 3 HLL cardinality: 14\n"
     ]
    }
   ],
   "source": [
    "# Get HLLSet for 3rd text\n",
    "text_3_hll = corpus_state.get_hllset_for_text(1)  # 0-indexed\n",
    "print(f\"Text 3 HLL cardinality: {text_3_hll.count():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Gets Stored\n",
    "\n",
    "Component |\tScope |\tPurpose\n",
    "----------|-------|---------\n",
    "text_hll |\tPer-text |\tSelf-generation loop input\n",
    "master_hll |\tGlobal |\tEnsures consistent hash_idx\n",
    "token_to_idx |\tGlobal |\tUnified adjacency matrix\n",
    "edge_freq |\tGlobal |\tAccumulates across ingestions\n",
    "lut\t| Global|\tToken → (reg, run) mapping\n",
    "state.hllsets |\tCollection\t| All text HLLSets\n",
    "\n",
    "**This design supports both**:\n",
    "\n",
    "✅ Self-generation: Each text has its own HLLSet\n",
    "\n",
    "✅ Global structure: Unified adjacency matrix across all texts\n",
    "\n",
    "✅ Incremental merge: Edge frequencies accumulate properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ingesting 2 texts ===\n",
      "Text 1: ⊢人工智能⊣\n",
      "  Text HLL cardinality: 13\n",
      "Text 2: ⊢机器学习⊣\n",
      "  Text HLL cardinality: 14\n",
      "\n",
      "Global state:\n",
      "  Total unique tokens: 32\n",
      "  Total HLLSets: 8\n",
      "  Total edges: 9\n",
      "  Master HLL cardinality: 32\n",
      "\n",
      "=== Saving corpus state ===\n",
      "Saved 32 token entries to LUT\n",
      "Saved AM: torch.Size([5, 5]), 9 edges, compressed to 0.5 KB\n",
      "Saved 8 HLLSets, compressed to 0.4 KB\n",
      "Save complete!\n",
      "\n",
      "\n",
      "=== Ingesting 2 texts ===\n",
      "Text 1: ⊢深度学习⊣\n",
      "  Text HLL cardinality: 12\n",
      "Text 2: ⊢神经网络⊣\n",
      "  Text HLL cardinality: 14\n",
      "\n",
      "Global state:\n",
      "  Total unique tokens: 43\n",
      "  Total HLLSets: 10\n",
      "  Total edges: 10\n",
      "  Master HLL cardinality: 44\n",
      "\n",
      "=== Saving corpus state ===\n",
      "Saved 43 token entries to LUT\n",
      "Saved AM: torch.Size([5, 5]), 10 edges, compressed to 0.5 KB\n",
      "Saved 10 HLLSets, compressed to 0.5 KB\n",
      "Save complete!\n",
      "\n",
      "\n",
      "=== Loading corpus state ===\n",
      "Loaded AM: torch.Size([5, 5]), 10 edges\n",
      "Loaded 10 HLLSets\n",
      "Loaded state: 10 texts, 43 tokens\n",
      "Load complete!\n",
      "\n",
      "Restored: 10 texts\n",
      "\n",
      "=== Ingesting 1 texts ===\n",
      "Text 1: ⊢计算机视觉⊣\n",
      "  Text HLL cardinality: 17\n",
      "\n",
      "Global state:\n",
      "  Total unique tokens: 56\n",
      "  Total HLLSets: 11\n",
      "  Total edges: 11\n",
      "  Master HLL cardinality: 57\n",
      "{'lut': {'total_entries': 43, 'unique_pairs': 43, 'collision_count': 0}, 'adj_size_kb': 0.4951171875, 'tok_id_size_kb': 0.25390625, 'hllsets_size_kb': 0.501953125, 'metadata': {'P': 10, 'total_texts': 10, 'total_tokens': 43, 'total_edges': 10, 'master_hll_cardinality': 44.0}}\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import hllset_swarm.persistence\n",
    "importlib.reload(hllset_swarm.persistence)\n",
    "from hllset_swarm.persistence import PersistenceManager\n",
    "\n",
    "# Now re-run your code\n",
    "pm = PersistenceManager(\"./sgs_state\")\n",
    "\n",
    "# ===== Iteration 1 =====\n",
    "corpus1 = [\"人工智能\", \"机器学习\"]\n",
    "corpus_state.ingest_corpus(corpus1)\n",
    "\n",
    "# Save after iteration completes\n",
    "pm.save(corpus_state)\n",
    "\n",
    "# ===== Iteration 2 (incremental) =====\n",
    "corpus2 = [\"深度学习\", \"神经网络\"]\n",
    "corpus_state.ingest_corpus(corpus2)\n",
    "\n",
    "# Save again (overwrites with updated state)\n",
    "pm.save(corpus_state)\n",
    "\n",
    "# ===== Restore from disk =====\n",
    "restored_state = pm.load(P=10)\n",
    "if restored_state:\n",
    "    print(f\"Restored: {len(restored_state.hllsets)} texts\")\n",
    "    \n",
    "    # Continue from where we left off\n",
    "    corpus3 = [\"计算机视觉\"]\n",
    "    corpus_state.ingest_corpus(corpus3)\n",
    "\n",
    "# Check storage stats\n",
    "stats = pm.get_stats()\n",
    "print(stats)\n",
    "\n",
    "pm.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hllset-swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
