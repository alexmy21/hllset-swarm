{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POC: pruned-AM iterative forecasting with d-/r-/n-hll guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Check available GPUs\n",
    "print(\"Available GPUs:\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    props = torch.cuda.get_device_properties(i)\n",
    "    print(f\"  GPU {i}: {props.name}\")\n",
    "    print(f\"    Compute Capability: sm_{props.major}{props.minor}\")\n",
    "    print(f\"    Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# Select RTX 3060 (adjust index based on output above)\n",
    "# Option A: Hide Quadro, only show RTX\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # Assuming RTX is at index 1\n",
    "DEVICE = torch.device(\"cuda:0\")  # Now index 0 refers to the RTX\n",
    "\n",
    "# Option B: Explicitly select by index (if you don't set CUDA_VISIBLE_DEVICES)\n",
    "# DEVICE = torch.device(\"cuda:1\")  # Direct access to RTX at original index 1\n",
    "\n",
    "print(f\"\\nUsing device: {DEVICE}\")\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(DEVICE)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(DEVICE).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "POC: pruned-AM iterative forecasting with d-/r-/n-hll guidance\n",
    "python am_forecast_poc.py\n",
    "\"\"\"\n",
    "import random, hashlib, json\n",
    "torch.manual_seed(42)\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "VOCAB  = 1_000          # toy vocabulary size\n",
    "MAX_EDGES = 5_000       # hard prune ceiling\n",
    "K = 3                   # tensor slices (τ, ρ, Δ)\n",
    "TOPK = 20               # flashlight circle\n",
    "\n",
    "# ---------- 1. toy corpus -> HLLSet covers ----------\n",
    "def fake_cover(n):\n",
    "    \"\"\"return sparse indices of n random tokens\"\"\"\n",
    "    return torch.unique(torch.randint(0, VOCAB, (n,)))\n",
    "\n",
    "corpus = [fake_cover(random.randint(10, 50)) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 2. sparse AM builder with pruning ----------\n",
    "from collections import defaultdict\n",
    "\n",
    "class PrunedAM:\n",
    "    \"\"\"CSR-like adjacency via torch sparse COO\"\"\"\n",
    "    def __init__(self):\n",
    "        self.edges = defaultdict(float)  # (u,v) → weight\n",
    "\n",
    "    def add_edge(self, u, v, w):\n",
    "        self.edges[(u, v)] += w\n",
    "\n",
    "    def csr(self):\n",
    "        if not self.edges:\n",
    "            return torch.sparse_coo_tensor(\n",
    "                torch.empty((2, 0), dtype=torch.long, device=DEVICE),\n",
    "                torch.empty(0, dtype=torch.float32, device=DEVICE),  # Force float32\n",
    "                size=(VOCAB, VOCAB),\n",
    "                device=DEVICE\n",
    "            )\n",
    "        row, col, val = [], [], []\n",
    "        for (u, v), w in self.edges.items():\n",
    "            row.append(u)\n",
    "            col.append(v)\n",
    "            val.append(w)\n",
    "        \n",
    "        indices = torch.tensor([row, col], dtype=torch.long, device=DEVICE)\n",
    "        values = torch.tensor(val, dtype=torch.float32, device=DEVICE)  # Force float32\n",
    "        \n",
    "        return torch.sparse_coo_tensor(\n",
    "            indices, \n",
    "            values, \n",
    "            size=(VOCAB, VOCAB),\n",
    "            dtype=torch.float32,  # Force float32\n",
    "            device=DEVICE\n",
    "        ).coalesce()\n",
    "    \n",
    "am = PrunedAM()\n",
    "for cover in corpus:\n",
    "    for u in cover:\n",
    "        for v in cover:\n",
    "            if u != v: am.add_edge(u, v, 1.0)   # τ-lattice\n",
    "Wτ = am.csr()\n",
    "Wρ = Wτ * 0.3                                    # ρ-lattice (scaled)\n",
    "W_prev = Wτ.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 3. relational tensor ----------\n",
    "class RelationalTensor:\n",
    "    \"\"\"K slices, each VOCAB×VOCAB sparse\"\"\"\n",
    "    def __init__(self, K, vocab_size, budget_per_slice):\n",
    "        self.K = K\n",
    "        self.vocab = vocab_size\n",
    "        self.budget = budget_per_slice\n",
    "        # Explicitly use float32 for all slices\n",
    "        self.slices = [\n",
    "            torch.sparse_coo_tensor(\n",
    "                size=(vocab_size, vocab_size), \n",
    "                dtype=torch.float32,  # Force float32\n",
    "                device=DEVICE\n",
    "            )\n",
    "            for _ in range(K)\n",
    "        ]\n",
    "\n",
    "    def overwrite_slice(self, k, sparse_mat):\n",
    "        # Ensure float32 dtype\n",
    "        sparse_mat = sparse_mat.to(dtype=torch.float32).coalesce()\n",
    "        \n",
    "        # Check budget\n",
    "        nnz = sparse_mat._nnz()\n",
    "        \n",
    "        if nnz > self.budget:\n",
    "            # Prune to budget by keeping top-k values\n",
    "            vals, idx = torch.topk(sparse_mat.values(), self.budget)\n",
    "            sparse_mat = torch.sparse_coo_tensor(\n",
    "                sparse_mat.indices()[:, idx], \n",
    "                vals,\n",
    "                size=sparse_mat.shape,\n",
    "                dtype=torch.float32,  # Force float32\n",
    "                device=DEVICE\n",
    "            ).coalesce()\n",
    "        \n",
    "        self.slices[k] = sparse_mat\n",
    "\n",
    "    def contract(self, k, belief_vec):\n",
    "        \"\"\"belief_vec → W[k] @ belief_vec\"\"\"\n",
    "        Wk = self.slices[k]\n",
    "        \n",
    "        # Ensure belief_vec is float32\n",
    "        belief_vec = belief_vec.to(dtype=torch.float32)\n",
    "        \n",
    "        # Sparse matrix-vector multiplication\n",
    "        result = torch.sparse.mm(Wk, belief_vec.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        return result.to(dtype=torch.float32)  # Ensure output is float32\n",
    "\n",
    "    def get_stats(self):\n",
    "        \"\"\"Return statistics about tensor state\"\"\"\n",
    "        stats = {\n",
    "            'K': self.K,\n",
    "            'vocab': self.vocab,\n",
    "            'budget_per_slice': self.budget,\n",
    "            'slices': []\n",
    "        }\n",
    "        \n",
    "        for k, slice_tensor in enumerate(self.slices):\n",
    "            slice_stats = {\n",
    "                'index': k,\n",
    "                'nnz': slice_tensor._nnz(),\n",
    "                'shape': slice_tensor.shape,\n",
    "                'dtype': str(slice_tensor.dtype),\n",
    "                'density': slice_tensor._nnz() / (self.vocab * self.vocab) if self.vocab > 0 else 0,\n",
    "                'memory_mb': (slice_tensor._nnz() * (8 + 8 + 4)) / 1024**2  # indices + values\n",
    "            }\n",
    "            stats['slices'].append(slice_stats)\n",
    "        \n",
    "        stats['total_nnz'] = sum(s['nnz'] for s in stats['slices'])\n",
    "        stats['total_memory_mb'] = sum(s['memory_mb'] for s in stats['slices'])\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "RT = RelationalTensor(K, VOCAB, MAX_EDGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Relational Tensor Stats ===\n",
      "Total non-zero entries: 0\n",
      "Total memory: 0.00 MB\n",
      "\n",
      "Per-slice breakdown:\n",
      "  Slice 0: 0 edges, density=0.000000\n",
      "  Slice 1: 0 edges, density=0.000000\n",
      "  Slice 2: 0 edges, density=0.000000\n"
     ]
    }
   ],
   "source": [
    "# Test the tensor operations\n",
    "print(\"\\n=== Relational Tensor Stats ===\")\n",
    "stats = RT.get_stats()\n",
    "print(f\"Total non-zero entries: {stats['total_nnz']}\")\n",
    "print(f\"Total memory: {stats['total_memory_mb']:.2f} MB\")\n",
    "print(\"\\nPer-slice breakdown:\")\n",
    "for s in stats['slices']:\n",
    "    print(f\"  Slice {s['index']}: {s['nnz']} edges, density={s['density']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 4. d-/r-/n-hll decomposition ----------\n",
    "def hll_delta(hll_t, hll_t1):\n",
    "    \"\"\"return sparse indices for d, r, n\"\"\"\n",
    "    set_t  = set(hll_t.cpu().tolist())\n",
    "    set_t1 = set(hll_t1.cpu().tolist())\n",
    "    d = torch.tensor(list(set_t - set_t1), dtype=torch.long, device=DEVICE)\n",
    "    r = torch.tensor(list(set_t & set_t1), dtype=torch.long, device=DEVICE)\n",
    "    n = torch.tensor(list(set_t1 - set_t), dtype=torch.long, device=DEVICE)\n",
    "    return d, r, n\n",
    "\n",
    "# ---------- 5. iterative forecast ----------\n",
    "def forecast(prompt_hll, max_iter=10, tol=1e-3):\n",
    "    \"\"\"\n",
    "    Iterative belief propagation over relational tensor.\n",
    "    \n",
    "    Args:\n",
    "        prompt_hll: Tensor of node indices in prompt HLL\n",
    "        max_iter: Maximum iterations\n",
    "        tol: Convergence tolerance\n",
    "    \n",
    "    Returns:\n",
    "        (top_indices, belief_vector)\n",
    "    \"\"\"\n",
    "    # Initialize belief vector (ensure float32)\n",
    "    p = torch.zeros(VOCAB, dtype=torch.float32, device=DEVICE)\n",
    "    p[prompt_hll] = 1.0\n",
    "    p = p / p.sum()\n",
    "    \n",
    "    for itr in range(max_iter):\n",
    "        p_old = p.clone()\n",
    "        \n",
    "        # Contract through all K slices\n",
    "        for k in range(K):\n",
    "            p = RT.contract(k, p)\n",
    "            \n",
    "            # Normalize to prevent explosion/vanishing\n",
    "            p_sum = p.sum()\n",
    "            if p_sum > 0:\n",
    "                p = p / p_sum\n",
    "        \n",
    "        # Stabilization check\n",
    "        diff = torch.norm(p - p_old, 1).item()\n",
    "        if diff < tol:\n",
    "            print(f\"  Converged at iteration {itr+1}, diff={diff:.6f}\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"  Max iterations ({max_iter}) reached\")\n",
    "    \n",
    "    # Get top-k predictions\n",
    "    top_idx = torch.topk(p, min(TOPK, VOCAB)).indices\n",
    "    \n",
    "    return top_idx, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 6. on-line Hebb update + prune ----------\n",
    "def ingest_and_forecast(new_cover, teacher_cover):\n",
    "    \"\"\"update AM, tensor, prune with d/r/n guide, forecast\"\"\"\n",
    "    global W_prev\n",
    "    \n",
    "    # ---- build new AM fragment ----\n",
    "    new_am = PrunedAM()\n",
    "    for u in new_cover:\n",
    "        for v in new_cover:\n",
    "            if u != v: \n",
    "                new_am.add_edge(u, v, 1.0)\n",
    "    \n",
    "    Wτ_new = new_am.csr()  # Already float32 from PrunedAM.csr()\n",
    "\n",
    "    # ---- d/r/n guided prune ----\n",
    "    d, r, n = hll_delta(new_cover, teacher_cover)\n",
    "    \n",
    "    # Create set of nodes to keep (r and n)\n",
    "    keep_nodes = torch.cat([r, n]) if len(r) > 0 or len(n) > 0 else torch.tensor([], dtype=torch.long, device=DEVICE)\n",
    "    \n",
    "    if len(keep_nodes) > 0:\n",
    "        # Convert to set for fast lookup\n",
    "        keep_set = set(keep_nodes.cpu().tolist())\n",
    "        \n",
    "        # Filter edges: keep only if source OR target is in keep_nodes\n",
    "        Wτ_new = Wτ_new.coalesce()\n",
    "        indices = Wτ_new.indices()\n",
    "        values = Wτ_new.values()\n",
    "        \n",
    "        # Create mask: True if row OR col is in keep_set\n",
    "        row_mask = torch.tensor([idx.item() in keep_set for idx in indices[0]], \n",
    "                                dtype=torch.bool, device=DEVICE)\n",
    "        col_mask = torch.tensor([idx.item() in keep_set for idx in indices[1]], \n",
    "                                dtype=torch.bool, device=DEVICE)\n",
    "        edge_mask = row_mask | col_mask\n",
    "        \n",
    "        # Apply mask\n",
    "        filtered_indices = indices[:, edge_mask]\n",
    "        filtered_values = values[edge_mask]\n",
    "        \n",
    "        Wτ_new = torch.sparse_coo_tensor(\n",
    "            filtered_indices,\n",
    "            filtered_values,\n",
    "            size=(VOCAB, VOCAB),\n",
    "            dtype=torch.float32,  # Force float32\n",
    "            device=DEVICE\n",
    "        ).coalesce()\n",
    "    else:\n",
    "        # No nodes to keep, create empty sparse tensor\n",
    "        Wτ_new = torch.sparse_coo_tensor(\n",
    "            torch.empty((2, 0), dtype=torch.long, device=DEVICE),\n",
    "            torch.empty(0, dtype=torch.float32, device=DEVICE),  # Force float32\n",
    "            size=(VOCAB, VOCAB),\n",
    "            device=DEVICE\n",
    "        )\n",
    "\n",
    "    # ---- tensor hot-swap ----\n",
    "    RT.overwrite_slice(0, Wτ_new)\n",
    "    RT.overwrite_slice(1, Wτ_new * 0.3)\n",
    "    \n",
    "    # Ensure W_prev is float32\n",
    "    W_prev = W_prev.to(dtype=torch.float32)\n",
    "    delta = (Wτ_new.to_dense() - W_prev.to_dense()).to_sparse().coalesce()\n",
    "    \n",
    "    RT.overwrite_slice(2, delta)\n",
    "    W_prev = Wτ_new\n",
    "\n",
    "    # ---- forecast ----\n",
    "    response, belief = forecast(new_cover)\n",
    "    return response, belief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Converged at iteration 2, diff=0.000000\n",
      "Response tokens (top-20): [19, 18, 16, 17, 1, 0, 2, 3, 11, 10, 8, 9, 13, 12, 14, 15, 7, 6, 4, 5]\n",
      "Belief vector sparsity: 0 / 1000\n"
     ]
    }
   ],
   "source": [
    "# ---------- 7. demo run ----------\n",
    "if __name__ == \"__main__\":\n",
    "    user_prompt = fake_cover(30)\n",
    "    teacher     = fake_cover(35)        # simulated host revision\n",
    "    resp, belief = ingest_and_forecast(user_prompt, teacher)\n",
    "    print(\"Response tokens (top-20):\", resp.cpu().tolist())\n",
    "    print(\"Belief vector sparsity:\", (belief>0).sum().item(), \"/\", VOCAB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hllset-swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
