{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AM-Guided Disambiguation Demo\n",
    "\n",
    "Demonstrates the new adjacency matrix-guided disambiguation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Imported binding MainInclude.eval was undeclared at import time during import to Main.\n",
      "WARNING: import of MainInclude.eval into Main conflicts with an existing identifier; ignored.\n",
      "WARNING: Imported binding MainInclude.include was undeclared at import time during import to Main.\n",
      "WARNING: import of MainInclude.include into Main conflicts with an existing identifier; ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HLLSet kernel from: None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "   \n",
    "from hllset_swarm.ingest import CorpusState\n",
    "from hllset_swarm.disambiguate import disambiguate_with_am \n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Simple Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ingesting 1 texts ===\n",
      "Text 1: ⊢人工智能⊣\n",
      "  Text HLL cardinality: 13\n",
      "\n",
      "Global state:\n",
      "  Total unique tokens: 13\n",
      "  Total HLLSets: 1\n",
      "  Total edges: 5\n",
      "  Master HLL cardinality: 13\n",
      "Original text: 人工智能\n",
      "HLL cardinality: 13\n",
      "AM size: torch.Size([4, 4]), 5 edges\n"
     ]
    }
   ],
   "source": [
    "# Create corpus\n",
    "corpus = [\"人工智能\"]\n",
    "\n",
    "# Ingest\n",
    "state = CorpusState(P=10)\n",
    "state.ingest_corpus(corpus)\n",
    "\n",
    "# Get components\n",
    "adj, token_to_idx = state.get_adjacency_matrix()\n",
    "text_hll = state.get_hllset_for_text(0)\n",
    "print(f\"Original text: {corpus[0]}\")\n",
    "print(f\"HLL cardinality: {text_hll.count():.0f}\")\n",
    "print(f\"AM size: {adj.shape}, {adj._nnz()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting AM-guided disambiguation ===\n",
      "START symbol: '⊢'\n",
      "END symbol: '⊣'\n",
      "\n",
      "[Path 1] Current: '⊢', Collected: []\n",
      "  Valid 2-grams: []\n",
      "  ✗ Dead end (no valid 2-grams)\n",
      "\n",
      "=== Disambiguation complete ===\n",
      "Explored 1 paths\n",
      "Found 0 valid sequences\n",
      "\\n==================================================\n",
      "Found 0 sequences:\n",
      "\\nBest sequence: None\n",
      "Original:      人工智能\n",
      "Match: False\n"
     ]
    }
   ],
   "source": [
    "# Disambiguate\n",
    "sequences, best = disambiguate_with_am(\n",
    "    text_hll, adj, token_to_idx, state.lut, max_paths=10\n",
    ")\n",
    "\n",
    "print(f\"\\\\n{'='*50}\")\n",
    "print(f\"Found {len(sequences)} sequences:\")\n",
    "\n",
    "for i, seq in enumerate(sequences, 1):\n",
    "    print(f\"  {i}. {''.join(seq)}\")\n",
    "\n",
    "print(f\"\\\\nBest sequence: {''.join(best) if best else 'None'}\")\n",
    "print(f\"Original:      {corpus[0]}\")\n",
    "print(f\"Match: {best == list(corpus[0]) if best else False}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Multiple Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2 = [\"人工智能\", \"机器学习\", \"深度学习\"]\n",
    "\n",
    "state2 = CorpusState(P=10)\n",
    "state2.ingest_corpus(corpus2)\n",
    "\n",
    "adj2, token_to_idx2 = state2.get_adjacency_matrix()\n",
    "\n",
    "print(f\"Corpus: {corpus2}\")\n",
    "print(f\"Total tokens: {len(token_to_idx2)}\")\n",
    "print(f\"Total edges: {adj2._nnz()}\")\n",
    "print(f\"Master HLL: {state2.master_hll.count():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disambiguate each text\n",
    "for i, original_text in enumerate(corpus2):\n",
    "    hll = state2.get_hllset_for_text(i)\n",
    "    sequences, best = disambiguate_with_am(\n",
    "        hll, adj2, token_to_idx2, state2.lut, max_paths=20\n",
    "    )\n",
    "\n",
    "    recovered = ''.join(best) if best else 'FAILED'\n",
    "    match = '✓' if best == list(original_text) else '✗'\n",
    "\n",
    "    print(f\"\\\\nText {i+1}:\")\n",
    "    print(f\"  Original:  {original_text}\")\n",
    "    print(f\"  Recovered: {recovered} {match}\")\n",
    "    print(f\"  Paths explored: {len(sequences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create larger corpus\n",
    "large_corpus = [\n",
    "    \"人工智能是计算机科学的一个分支\",\n",
    "    \"机器学习是实现人工智能的一种方法\",\n",
    "    \"深度学习是机器学习的一个子领域\",\n",
    "    \"神经网络是深度学习的核心技术\"\n",
    "]\n",
    "\n",
    "state3 = CorpusState(P=12)\n",
    "state3.ingest_corpus(large_corpus)\n",
    "\n",
    "adj3, token_to_idx3 = state3.get_adjacency_matrix()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hllset-swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
