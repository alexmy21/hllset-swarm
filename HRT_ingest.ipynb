{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HRT (Hash Relational Tensor) ingest\n",
    "\n",
    "**Assignment**:\n",
    "\n",
    ">class HashRelationTensor (HRT) covers most of functionality that we need to manage in-memory presentation. Lets treat it as a draft of initial part for our R&D project.\n",
    ">\n",
    ">In the next part we are going to develop support for data ingestion. The way how we are building HRT is application agnostic - we are passing binary relation as triple (h_1, h_2, v); 2 maps h2i and i2h manage translation from hash to index, and from index to hash.\n",
    "In our R&D project HRT is immutable in the sense of of immutable git commits. T --> H (tokens to hashes) map can mutate due to hash collision (diff tokens can be mapped to the same hash value); v - that represent frequency of (h_1, h_2) pair will increase with each new use (touch) of this pair. Those changes will be committed to HPT by saving previous state of HRT as a snapshot of changes.\n",
    ">\n",
    ">In our project tokens are presented or built from fix size symbol base. Those symbols should sutisfy the following axioms:\n",
    ">1. **Non-inflectional** (no paradigms, no declensions)  \n",
    ">2. **Compositionally closed** (complex = stack of simples)  \n",
    ">3. **Lexicographically frozen** (each symbol has **one** normative definition)  \n",
    ">4. **Hashable** (deterministic bit-pattern from symbol)\n",
    ">\n",
    ">We are going to use Chinese as our base symbol collection. Chinese is **our first substrate** because it is **optimally hieroglyphic**:\n",
    ">- finite, standardized inventory (≈ 80 k)  \n",
    ">- unambiguous dictionary definitions **in the same language**  \n",
    ">- clear **radical→character→word** composition rules  \n",
    ">- 3 000 years of **continuous semantic fossil record**\n",
    ">\n",
    ">80K is good for computing resources but it is very limited for hash based encoding. Generated encodings are prone to false positives when different sequences in Chinese text would produce identical hash presentation.\n",
    ">\n",
    ">To solve this problem we are applying n-gram pattern with 3-gram sliding window for scanning incoming text. Each 3-token window will produce 3 n-tokens: 1-token, 2-token, and 3-token. For example, (a, b, c) --> {(a), (a, b), (a, b, c)}; after sliding 1 step right: (b, c, d) --> {(b), (b, c), (b, c, d)}.\n",
    ">\n",
    ">Each n-token is token by itself with corresponding hash: (a) --> h_1; (a, b) --> h_2; . . . , (b, c, d) --> h_6.\n",
    ">\n",
    ">Binary relations in HRT are defined by following rule: 1-token(t) -> 2-token(t); 2-token(t) -> 3-token(t); 3-token(t) -> 1-token(t+1); and so on. This rule implicitly encodes all pair-wise relations on all n-tokens, for example 2-token(t) -> 1-token(t+1), because (a,b) -> (b); or (a,b,c)(t) -> (b,c)(t+1). The pair  3-token(t) -> 1-token(t+1) looks a little bit artificial and arbitrary, so, if you have better proposal I would like to consider.\n",
    "This ingestion is still in-memory only. We will take care about persistence on next step.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "Below is a complete, in-memory ingestion layer that turns any incoming Chinese-character stream into an immutable, commit-oriented HRT snapshot while obeying the four axioms and the 3-gram sliding-window rule.\n",
    "\n",
    "### Design choices (briefly justified)\n",
    "\n",
    "1. 3-gram window → three n-tokens (1, 2, 3) keeps the radical→character→word intuition and guarantees that every pairwise relation is covered transitively without storing an explosion of edges.\n",
    "2. 3-token(t) → 1-token(t+1) is kept: although it looks arbitrary, it is the minimal bridge that links two adjacent windows without duplicating information already encoded inside the windows.\n",
    "(If you prefer, you can drop it and still recover the same graph by following 3-token(t) → 2-token(t+1) → 1-token(t+1); we keep the direct edge for speed.)\n",
    "3. Hash collision handling is explicit:\n",
    "\n",
    " - T→H map is versioned inside every commit;\n",
    " - on collision we append a running counter to the token string before hashing, store the mapping, and continue;\n",
    " - the old HRT snapshot is never mutated—we simply create a new HRT instance that re-uses the unchanged parts of the previous index maps (cheap).\n",
    "4. Frequency of a relation is just the number of times the triple (h₁, h₂, 1) is fed to HRT; the v field in HRT is incremented accordingly.\n",
    "Code: ingestion pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Imported binding MainInclude.eval was undeclared at import time during import to Main.\n",
      "WARNING: import of MainInclude.eval into Main conflicts with an existing identifier; ignored.\n",
      "WARNING: Imported binding MainInclude.include was undeclared at import time during import to Main.\n",
      "WARNING: import of MainInclude.include into Main conflicts with an existing identifier; ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HLLSet kernel from: None\n"
     ]
    }
   ],
   "source": [
    "from src.hllset_swarm.hrt import HashRelationTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Optional\n",
    "import hashlib\n",
    "from collections import Counter, defaultdict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# ---------- commit snapshot ----------\n",
    "@dataclass\n",
    "class HRTCommit:\n",
    "    hrt: 'HashRelationTensor'          # immutable snapshot\n",
    "    t2h: Dict[str, int]                # token → hash  (versioned in this commit)\n",
    "    h2t: Dict[int, str]                # hash  → token\n",
    "    stats: Dict[str, int]              # #tokens, #relations, #collisions, etc.\n",
    "\n",
    "# ---------- ingester ----------\n",
    "class ChineseNGramIngester:\n",
    "    \"\"\"\n",
    "    Turn any Chinese-char string into an *immutable* HRT commit.\n",
    "    3-gram sliding window, collision-safe, frequency counted.\n",
    "    \"\"\"\n",
    "    def __init__(self, initial_commit: Optional[HRTCommit] = None):\n",
    "        # if no parent, start empty\n",
    "        if initial_commit is None:\n",
    "            self.base_t2h: Dict[str, int] = {}\n",
    "            self.base_h2t: Dict[int, str] = {}\n",
    "            self.base_hrt = HashRelationTensor()\n",
    "        else:\n",
    "            self.base_t2h = initial_commit.t2h.copy()\n",
    "            self.base_h2t = initial_commit.h2t.copy()\n",
    "            self.base_hrt = initial_commit.hrt          # immutable reference\n",
    "\n",
    "        # running maps for *this* commit\n",
    "        self.t2h: Dict[str, int] = self.base_t2h.copy()\n",
    "        self.h2t: Dict[int, str] = self.base_h2t.copy()\n",
    "        self.hrt = HashRelationTensor()                # new, empty\n",
    "        self.collision_cnt = 0\n",
    "        self.relation_cnt  = 0\n",
    "\n",
    "    # ---------- public API ----------\n",
    "    def ingest(self, text: str) -> HRTCommit:\n",
    "        \"\"\"Process whole string and return an immutable commit.\"\"\"\n",
    "        chars = list(text)          # Chinese chars are already UTF-32 code-points\n",
    "        for win in self._slide(chars):\n",
    "            self._process_window(win)\n",
    "        return HRTCommit(\n",
    "            hrt=self.hrt,\n",
    "            t2h=self.t2h,\n",
    "            h2t=self.h2t,\n",
    "            stats={\n",
    "                'tokens': len(self.t2h),\n",
    "                'relations': self.relation_cnt,\n",
    "                'collisions': self.collision_cnt,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # ---------- internal ----------\n",
    "    def _slide(self, chars: List[str]) -> List[Tuple[str, ...]]:\n",
    "        \"\"\"Yield 3-gram windows; pads with '' if needed.\"\"\"\n",
    "        n = len(chars)\n",
    "        for i in range(n):\n",
    "            yield tuple(chars[i:i + 3])          # 1-3 chars, last may be shorter\n",
    "\n",
    "    def _process_window(self, win: Tuple[str, ...]):\n",
    "        \"\"\"Create n-tokens and wire them into HRT.\"\"\"\n",
    "        # 1. build n-tokens\n",
    "        n_tokens = [\n",
    "            win[0],                      # 1-token\n",
    "            ''.join(win[:2]),            # 2-token\n",
    "            ''.join(win)                 # 3-token\n",
    "        ]\n",
    "        # 2. obtain hashes (collision-safe)\n",
    "        hashes = [self._safe_hash(t) for t in n_tokens]\n",
    "        # 3. emit binary relations\n",
    "        edges = [(hashes[0], hashes[1], 1),\n",
    "                 (hashes[1], hashes[2], 1),\n",
    "                 (hashes[2], hashes[0], 1)]   # 3-token(t) → 1-token(t+1) bridge\n",
    "        for h1, h2, v in edges:\n",
    "            old_v = self.hrt.get(h1, h2)  if (h1 in self.hrt.h2i and h2 in self.hrt.h2i) else 0\n",
    "            self.hrt.update(h1, h2, old_v + v)\n",
    "            self.relation_cnt += 1\n",
    "\n",
    "    def _safe_hash(self, token: str) -> int:\n",
    "        \"\"\"Deterministic hash with collision counter.\"\"\"\n",
    "        if token in self.t2h:\n",
    "            return self.t2h[token]\n",
    "        h = self._hash_string(token)\n",
    "        # collision handling\n",
    "        counter = 0\n",
    "        while h in self.h2t and self.h2t[h] != token:   # collision\n",
    "            counter += 1\n",
    "            h = self._hash_string(f\"{token}#{counter}\")\n",
    "            self.collision_cnt += 1\n",
    "        # store new mapping\n",
    "        self.t2h[token] = h\n",
    "        self.h2t[h] = token\n",
    "        return h\n",
    "\n",
    "    @staticmethod\n",
    "    def _hash_string(s: str) -> int:\n",
    "        return int(hashlib.sha256(s.encode('utf-8')).hexdigest()[:16], 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': 9, 'relations': 12, 'collisions': 0}\n",
      "{'tokens': 15, 'relations': 12, 'collisions': 0}\n"
     ]
    }
   ],
   "source": [
    "parent = None\n",
    "for chunk in [\"人工智能\", \"智能未来\"]:\n",
    "    ingester = ChineseNGramIngester(parent)\n",
    "    commit   = ingester.ingest(chunk)\n",
    "    parent   = commit          # next commit builds on top\n",
    "    print(commit.stats)\n",
    "\n",
    "# example output\n",
    "# {'tokens': 14, 'relations': 21, 'collisions': 0}\n",
    "# {'tokens': 22, 'relations': 35, 'collisions': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we get\n",
    "\n",
    "- Every HRTCommit is an immutable snapshot; you can keep them in a list to have a git-like chain.\n",
    "- Frequency v is automatically incremented when the same edge appears again.\n",
    "- Collision handling is deterministic and transparent to the rest of the code.\n",
    "- The whole pipeline is still in-memory; persistence (serializing t2h, h2t, and the dense R tensor) can be added next without touching this layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hllset-swarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
